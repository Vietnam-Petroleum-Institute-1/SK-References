{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Reminder: This 📘 `Python` notebook needs can be run from VS Code with [these prerequisites](../PREREQS.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use this notebook: \n",
    "\n",
    "* Just read the text and scroll along until you run into code blocks.\n",
    "* Code blocks have computer code inside them — hover over the block and you can run the code.\n",
    "* Run the code by hitting the ▶️ \"play\" button to the left. If the code runs you'll see a ✔️. If not, you'll get a ❌.\n",
    "* The output and status of the code block will appear just below itself — you need to scroll down further to see it.\n",
    "* Sometimes a code block will ask you for input in a hard-to-notice dialog box 👆 at the top of your notebook window. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Recipe: 💬 MiniChatGPT Clone\n",
    "## 🧑‍🍳 Cook a simple \"ChatGPT\" clone\n",
    "\n",
    "The magic of chat using LLM AI is that it's extremely simple to implement. If you're still feeling unsure about semantic functions, catch up [here](https://learn.microsoft.com/en-us/semantic-kernel/howto/semanticfunctions). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Let's get started by instantiating a 🔥 kernel\n",
    "\n",
    "You've already set up your API key information, so this should be an easy ▶️ and you're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.3.3.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, OpenAITextCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureTextCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAITextCompletion(\"text-davinci-003\", api_key, org_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "😱 **Get an error message?** The [first notebook](../s1e1-ez-starter-notebook/notebook.ipynb) walks you through this process so you should be all set. But if you're still stuck, go to https://aka.ms/sk/discord where we have realtime support."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2️: Use the `ChatSkill` 🧂 skill to build a chatting AI in no time\n",
    "\n",
    "We will be using the `Chat` and `ChatPersona` semantic functions that are accessible from within the `ChatSkill` subdirectory of `skills`\n",
    "\n",
    "```\n",
    "🗂️ skills\n",
    "│\n",
    "└─── 📁 FunSkill\n",
    "└─── 📁 ChatSkill\n",
    "     |\n",
    "     └─── 📂 Chat\n",
    "     └─── 📂 ChatPersona\n",
    "```\n",
    "\n",
    "And with that, let's get the LLM AI to tell us talk to us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Create a simple chat loop using a basic kind of 🥑 memory\n",
    "\n",
    "You'll be surprised to see how easy it is to make AI chat happen with the semantic function `ChatSkill.Chat.` It's easy as `history += <the new chat exchange>`! Or in other words, create a prompt that is incrementally just your running conversation that gets fed into the prompt each turn of the chat. The LLM's job then simply becomes the task to auto-complete what should get said next. The context is a kind of persistent 🥑 memory that gradually increases the richness of the prompt as more conversation is fed into `history.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Skills Directory\n",
    "skillsDirectory = \"./skills/\"\n",
    "\n",
    "# Load the Chat function from the FunSkill\n",
    "chatSkill = kernel.import_semantic_skill_from_directory(skillsDirectory, \"ChatSkill\")\n",
    "\n",
    "myContext = kernel.create_new_context()\n",
    "botPrompt = \"AI: Hello. What's your name?\"\n",
    "history = \"{botPrompt}\\n\"\n",
    "\n",
    "numberOfRounds = 4\n",
    "myContext[\"history\"] = history\n",
    "\n",
    "for i in range(numberOfRounds):\n",
    "    try:\n",
    "        # get input from the user and set the context variable\n",
    "        print(\"👆 Enter text in the input cell above to chat with the bot. 👆\\n\")\n",
    "        sk_input = input(f\"{botPrompt} ({(i+1)} of {numberOfRounds})\")\n",
    "        myContext[\"input\"] = sk_input\n",
    "\n",
    "        # run the chat function\n",
    "        myResult = await kernel.run_async(chatSkill[\"Chat\"], input_context=myContext)\n",
    "\n",
    "        # tack onto the history 👇 what's come back from the model\n",
    "        # ********************************************************\n",
    "        theNewChatExchange = f\"Me: {sk_input}\\nAI:{myResult}\\n\"\n",
    "        history += theNewChatExchange\n",
    "        myContext[\"history\"] = history\n",
    "\n",
    "        # ********************************************************\n",
    "        # this way the new chat exchange gets passed into the next round\n",
    "\n",
    "        # announce the number of rounds and the history\n",
    "        print(f\"Chat for {i+1} of {numberOfRounds} rounds with AI:\\n{history}\")\n",
    "\n",
    "        # prepare to \"prompt\" the user with the bot's response\n",
    "        botPrompt = f\"AI: {myResult}\"\n",
    "    except:\n",
    "        # if the user hits \"Escape\" we end the chat early\n",
    "        print(\"AI: Thanks for the wonderful chat!\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ✅ Note that the chat appears in the output above as you enter text in the little textbox that appears at the top of VS Code. It will stop after you've gone for `numberOfRounds` — so increase the number of rounds you'd like to chat with YOUR bot! You made it yourself!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Create a chat loop using 🥑 memory and with a personality of your choice\n",
    "\n",
    "The accrual of `history += theNewChatExchange` is a beautiful thing to admire. This is the basic mechanism whereby the chats you have with ChatGPT can feel so realistic. That's because the prompt is getting slightly longer and longer as you chat. The reason \"it gets you\" is because it hasn't forgotten what it's chatted with you in the past. It's the extra 🥑 \"fat\" that makes the interaction with the AI so much more appealing.\n",
    "\n",
    "Yet another facet to consider is how you're able to change the personality of the bot so easily. We'll use the `ChatSkill.ChatPersonality` function to do so with its added `$personality` context variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Skills Directory\n",
    "skillsDirectory = \"./skills/\"\n",
    "\n",
    "# Load the Chat function from the FunSkill\n",
    "chatSkill = kernel.import_semantic_skill_from_directory(skillsDirectory, \"ChatSkill\")\n",
    "\n",
    "myContext = kernel.create_new_context() \n",
    "botName = \"AI\"\n",
    "\n",
    "# Choose a personality here 👇 ...\n",
    "# *********************************************************************/\n",
    "personality = \"grumpy and extremely unhelpful\"\n",
    "# *********************************************************************/\n",
    "\n",
    "botPrompt = f\"AI: Hello. My responses will be {personality}.\"\n",
    "history = f\"{botPrompt}\\n\"\n",
    "\n",
    "numberOfRounds = 4\n",
    "\n",
    "myContext[\"history\"] = history\n",
    "myContext[\"personality\"] = personality\n",
    "\n",
    "for i in range(numberOfRounds):\n",
    "    try:\n",
    "        # get input from the user and set the context variable\n",
    "        print(\"👆 Enter text in the input cell above to chat with the bot. 👆\\n\")\n",
    "        sk_input = input(f\"{botPrompt} ({(i+1)} of {numberOfRounds})\")\n",
    "        myContext[\"input\"] = sk_input\n",
    "\n",
    "        # run the chat function\n",
    "        myResult = await kernel.run_async(chatSkill[\"Chat\"], input_context=myContext)\n",
    "\n",
    "        # tack onto the history 👇 what's come back from the model\n",
    "        # ********************************************************\n",
    "        theNewChatExchange = f\"Me: {sk_input}\\nAI:{myResult}\\n\"\n",
    "        history += theNewChatExchange\n",
    "        myContext[\"history\"] = history\n",
    "\n",
    "        # ********************************************************\n",
    "        # this way the new chat exchange gets passed into the next round\n",
    "\n",
    "        # announce the number of rounds and the history\n",
    "        print(f\"Chat for {i+1} of {numberOfRounds} rounds with AI:\\n{history}\")\n",
    "\n",
    "        # prepare to \"prompt\" the user with the bot's response\n",
    "        botPrompt = f\"AI: {myResult}\"\n",
    "    except:\n",
    "        # if the user hits \"Escape\" we end the chat early\n",
    "        print(\"AI: Thanks for the wonderful chat!\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⏭️ Next Steps\n",
    "\n",
    "Run through more advanced examples in the notebooks that are available in our GitHub repo at [https://aka.ms/sk/repo](https://aka.ms/sk/repo).\n",
    "\n",
    "[Want to deepen your \"prompt engineer\" chops? Go deeper on semantic AI 🧂🔥! ](../e8-bonus-prompts/notebook.ipynb)\n",
    "\n",
    "Or stay a longer while and modify the `numberOfRounds` parameter for a longer chat. Even better, chang ethe `personality` parameter in Step 2.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
