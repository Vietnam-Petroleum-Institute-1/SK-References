
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Distributed Tensorflow &amp; Keras &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="https://docs.ray.io/en/master/_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="https://docs.ray.io/en/master/_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="https://docs.ray.io/en/master/_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="https://docs.ray.io/en/master/_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="https://docs.ray.io/en/master/_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/pygments.css" />
    <link rel="stylesheet" href="https://docs.ray.io/en/master/_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="https://docs.ray.io/en/master/_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="https://docs.ray.io/en/master/_static/documentation_options.js"></script>
    <script src="https://docs.ray.io/en/master/_static/jquery.js"></script>
    <script src="https://docs.ray.io/en/master/_static/underscore.js"></script>
    <script src="https://docs.ray.io/en/master/_static/doctools.js"></script>
    <script src="https://docs.ray.io/en/master/_static/clipboard.min.js"></script>
    <script src="https://docs.ray.io/en/master/_static/copybutton.js"></script>
    <script src="https://docs.ray.io/en/master/_static/js/versionwarning.js"></script>
    <script src="https://docs.ray.io/en/master/_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/docsearch.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/csat.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/termynal.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/custom.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/top-navigation.js"></script>
    <script src="https://docs.ray.io/en/master/_static/js/tags.js"></script>
    <script src="https://docs.ray.io/en/master/_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://docs.ray.io/en/master/_static/design-tabs.js"></script>
    <script async="async" src="https://docs.ray.io/_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/train/distributed-tensorflow-keras.html" />
    <link rel="shortcut icon" href="https://docs.ray.io/en/master/_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed XGBoost and LightGBM" href="distributed-xgboost-lightgbm.html" />
    <link rel="prev" title="More Frameworks" href="more-frameworks.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="https://docs.ray.io/_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": false, "api_host": "https://readthedocs.com", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-2", "language": "en", "page": "train/distributed-tensorflow-keras", "programming_language": "py", "project": "anyscale-ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="https://docs.ray.io/_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="train.html">
   Ray Train
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="key-concepts.html">
     Key Concepts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="distributed-pytorch.html">
     Distributed PyTorch
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="more-frameworks.html">
     More Frameworks
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="distributed-tensorflow-keras.html#">
       Distributed Tensorflow &amp; Keras
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="distributed-xgboost-lightgbm.html">
       Distributed XGBoost and LightGBM
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="horovod.html">
       Horovod
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="internals/index.html">
     Ray Train Internals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="examples.html">
     Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="faq.html">
     Ray Train FAQ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="api/api.html">
     Ray Train API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2Ftrain/distributed-tensorflow-keras.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/edit/master/doc/source/train/distributed-tensorflow-keras.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://docs.ray.io/en/master/_sources/train/distributed-tensorflow-keras.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#quickstart">
   Quickstart
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#updating-your-training-function">
   Updating your training function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#creating-a-tensorflowtrainer">
   Creating a
   <code class="xref py py-class docutils literal notranslate">
    <span class="pre">
     TensorflowTrainer
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#running-your-training-function">
   Running your training function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#data-loading-and-preprocessing">
   Data loading and preprocessing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#reporting-results">
   Reporting results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#aggregating-results">
     Aggregating results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#saving-and-loading-checkpoints">
   Saving and loading checkpoints
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#loading-checkpoints">
     Loading checkpoints
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#further-reading">
   Further reading
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Distributed Tensorflow & Keras</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#quickstart">
   Quickstart
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#updating-your-training-function">
   Updating your training function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#creating-a-tensorflowtrainer">
   Creating a
   <code class="xref py py-class docutils literal notranslate">
    <span class="pre">
     TensorflowTrainer
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#running-your-training-function">
   Running your training function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#data-loading-and-preprocessing">
   Data loading and preprocessing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#reporting-results">
   Reporting results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#aggregating-results">
     Aggregating results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#saving-and-loading-checkpoints">
   Saving and loading checkpoints
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#loading-checkpoints">
     Loading checkpoints
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="distributed-tensorflow-keras.html#further-reading">
   Further reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="distributed-tensorflow-keras">
<span id="train-tensorflow-overview"></span><h1>Distributed Tensorflow &amp; Keras<a class="headerlink" href="distributed-tensorflow-keras.html#distributed-tensorflow-keras" title="Permalink to this headline">#</a></h1>
<p>Ray Train’s <a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a> integration enables you
to scale your TensorFlow and Keras training loops to many machines and GPUs.</p>
<p>On a technical level, Ray Train schedules your training workers
and configures <code class="docutils literal notranslate"><span class="pre">TF_CONFIG</span></code> for you, allowing you to run
your <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> training script. See <a class="reference external" href="https://www.tensorflow.org/guide/distributed_training">Distributed
training with TensorFlow</a>
for more information.</p>
<p>Most of the examples in this guide use Tensorflow with Keras, but
Ray Train also works with vanilla Tensorflow.</p>
<section id="quickstart">
<h2>Quickstart<a class="headerlink" href="distributed-tensorflow-keras.html#quickstart" title="Permalink to this headline">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.air.integrations.keras</span> <span class="kn">import</span> <span class="n">ReportCheckpointCallback</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span>


<span class="c1"># If using GPUs, set this to True.</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">a</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">100</span>


<span class="k">def</span> <span class="nf">build_model</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">()),</span>
            <span class="c1"># Add feature dimension, expanding (batch_size,) to (batch_size, 1).</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
        <span class="c1"># Model building/compiling need to be within `strategy.scope()`.</span>
        <span class="n">multi_worker_model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
        <span class="n">multi_worker_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)),</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">tf_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">to_tf</span><span class="p">(</span>
            <span class="n">feature_columns</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">label_columns</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
        <span class="p">)</span>
        <span class="n">history</span> <span class="o">=</span> <span class="n">multi_worker_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="n">tf_dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">ReportCheckpointCallback</span><span class="p">()]</span>
        <span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>


<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span> <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">from_items</span><span class="p">(</span>
    <span class="p">[{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x</span> <span class="o">/</span> <span class="mi">200</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">/</span> <span class="mi">200</span><span class="p">}</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)]</span>
<span class="p">)</span>
<span class="n">scaling_config</span> <span class="o">=</span> <span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_loop_per_worker</span><span class="o">=</span><span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">scaling_config</span><span class="p">,</span>
    <span class="n">datasets</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">train_dataset</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="updating-your-training-function">
<h2>Updating your training function<a class="headerlink" href="distributed-tensorflow-keras.html#updating-your-training-function" title="Permalink to this headline">#</a></h2>
<p>First, you’ll want to update your training function to support distributed
training.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The current TensorFlow implementation supports
<code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code> (and <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code>). If there are
other strategies you wish to see supported by Ray Train, please let us know
by submitting a <a class="reference external" href="https://github.com/ray-project/ray/issues">feature request on GitHub</a>.</p>
</div>
<p>These instructions closely follow TensorFlow’s <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras">Multi-worker training
with Keras</a>
tutorial. One key difference is that Ray Train will handle the environment
variable set up for you.</p>
<p><strong>Step 1:</strong> Wrap your model in <code class="docutils literal notranslate"><span class="pre">MultiWorkerMirroredStrategy</span></code>.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy">MultiWorkerMirroredStrategy</a>
enables synchronous distributed training. The <code class="docutils literal notranslate"><span class="pre">Model</span></code> <em>must</em> be built and
compiled within the scope of the strategy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># build model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Step 2:</strong> Update your <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> batch size to the <em>global</em> batch
size.</p>
<p>The <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch">batch</a>
will be split evenly across worker processes, so <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> should be
set appropriately.</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="gd">-batch_size = worker_batch_size</span><span class="w"></span>
<span class="gi">+batch_size = worker_batch_size * train.get_context().get_world_size()</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Ray will not automatically set any environment variables or configuration
related to local parallelism / threading
<a class="reference internal" href="../ray-core/configure.html#omp-num-thread-note"><span class="std std-ref">aside from “OMP_NUM_THREADS”</span></a>.
If you desire greater control over TensorFlow threading, use
the <code class="docutils literal notranslate"><span class="pre">tf.config.threading</span></code> module (eg.
<code class="docutils literal notranslate"><span class="pre">tf.config.threading.set_inter_op_parallelism_threads(num_cpus)</span></code>)
at the beginning of your <code class="docutils literal notranslate"><span class="pre">train_loop_per_worker</span></code> function.</p>
</div>
</section>
<section id="creating-a-tensorflowtrainer">
<h2>Creating a <a class="reference internal" href="api/doc/ray.train.tensorflow.TensorflowTrainer.html#ray.train.tensorflow.TensorflowTrainer" title="ray.train.tensorflow.TensorflowTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorflowTrainer</span></code></a><a class="headerlink" href="distributed-tensorflow-keras.html#creating-a-tensorflowtrainer" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code>s are the primary Ray Train classes that are used to manage state and
execute training. For distributed Tensorflow,
we use a <a class="reference internal" href="api/doc/ray.train.tensorflow.TensorflowTrainer.html#ray.train.tensorflow.TensorflowTrainer" title="ray.train.tensorflow.TensorflowTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorflowTrainer</span></code></a>
that you can setup like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span>
<span class="c1"># For GPU Training, set `use_gpu` to True.</span>
<span class="n">use_gpu</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">use_gpu</span><span class="o">=</span><span class="n">use_gpu</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>To customize the backend setup, you can pass a
<a class="reference internal" href="api/doc/ray.train.tensorflow.TensorflowConfig.html#ray.train.tensorflow.TensorflowConfig" title="ray.train.tensorflow.TensorflowConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorflowConfig</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span><span class="p">,</span> <span class="n">TensorflowConfig</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">tensorflow_backend</span><span class="o">=</span><span class="n">TensorflowConfig</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For more configurability, please reference the <a class="reference internal" href="api/doc/ray.train.data_parallel_trainer.DataParallelTrainer.html#ray.train.data_parallel_trainer.DataParallelTrainer" title="ray.train.data_parallel_trainer.DataParallelTrainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallelTrainer</span></code></a> API.</p>
</section>
<section id="running-your-training-function">
<h2>Running your training function<a class="headerlink" href="distributed-tensorflow-keras.html#running-your-training-function" title="Permalink to this headline">#</a></h2>
<p>With a distributed training function and a Ray Train <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>, you are now
ready to start training!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="data-loading-and-preprocessing">
<h2>Data loading and preprocessing<a class="headerlink" href="distributed-tensorflow-keras.html#data-loading-and-preprocessing" title="Permalink to this headline">#</a></h2>
<p>Tensorflow per default uses its own internal dataset sharding policy, as described
<a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding">in the guide</a>.
If your tensorflow dataset is compatible with distributed loading, you don’t need to
change anything.</p>
<p>If you require more advanced preprocessing, you may want to consider using Ray Data
for distributed data ingest.</p>
<p>There is a guide for using <a class="reference internal" href="distributed-pytorch/data-loading-preprocessing.html#data-ingest-torch"><span class="std std-ref">Ray Data with Ray Train</span></a>
in our PyTorch guide. Since Ray Data is an independent library, most concepts can
be directly applied to TensorFlow.</p>
<p>The main difference is that you may want to convert your Ray Data dataset shard to
a TensorFlow dataset in your training function so that you can use the Keras
API for model training.</p>
<p><a class="reference external" href="https://github.com/ray-project/ray/blob/master/python/ray/train/examples/tf/tune_tensorflow_autoencoder_example.py">Here’s a full example you can refer to</a>
for distributed data loading. The relevant parts are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">prepare_dataset_shard</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="c1"># ...</span>

    <span class="c1"># Get dataset shard from Ray Train</span>
    <span class="n">dataset_shard</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_dataset_shard</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

    <span class="c1"># Define a helper function to build a TensorFlow dataset</span>
    <span class="k">def</span> <span class="nf">to_tf_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">to_tensor_iterator</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iter_tf_batches</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtypes</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">):</span>
                <span class="k">yield</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span>

        <span class="n">output_signature</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">tf_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_generator</span><span class="p">(</span>
            <span class="n">to_tensor_iterator</span><span class="p">,</span> <span class="n">output_signature</span><span class="o">=</span><span class="n">output_signature</span>
        <span class="p">)</span>
        <span class="c1"># Call prepare_dataset_shard to disable automatic sharding</span>
        <span class="c1"># (since the dataset is already sharded)</span>
        <span class="k">return</span> <span class="n">prepare_dataset_shard</span><span class="p">(</span><span class="n">tf_dataset</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Call our helper function to build the dataset</span>
        <span class="n">tf_dataset</span> <span class="o">=</span> <span class="n">to_tf_dataset</span><span class="p">(</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">dataset_shard</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">history</span> <span class="o">=</span> <span class="n">multi_worker_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tf_dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="reporting-results">
<h2>Reporting results<a class="headerlink" href="distributed-tensorflow-keras.html#reporting-results" title="Permalink to this headline">#</a></h2>
<p>During training, the training loop should report intermediate results and checkpoints
to Ray Train. This will log the results to the console output and append them to
local log files. It can also be used to report results to
<a class="reference internal" href="distributed-pytorch/experiment-tracking.html#train-monitoring"><span class="std std-ref">experiment tracking services</span></a> and it will trigger
<a class="reference internal" href="distributed-pytorch/checkpoints.html#train-dl-configure-checkpoints"><span class="std std-ref">checkpoint bookkeeping</span></a>.</p>
<p>The easiest way to report your results with Keras is by using the
<code class="xref py py-class docutils literal notranslate"><span class="pre">ReportCheckpointCallback</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray.air.integrations.keras</span> <span class="kn">import</span> <span class="n">ReportCheckpointCallback</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">ReportCheckpointCallback</span><span class="p">()])</span>
</pre></div>
</div>
<p>This callback will automatically forward all results and checkpoints from the
Keras training loop to Ray Train.</p>
<section id="aggregating-results">
<h3>Aggregating results<a class="headerlink" href="distributed-tensorflow-keras.html#aggregating-results" title="Permalink to this headline">#</a></h3>
<p>TensorFlow Keras automatically aggregates metrics from all workers. If you wish to have more
control over that, consider implementing a <a class="reference external" href="https://www.tensorflow.org/tutorials/distribute/custom_training">custom training loop</a>.</p>
</section>
</section>
<section id="saving-and-loading-checkpoints">
<h2>Saving and loading checkpoints<a class="headerlink" href="distributed-tensorflow-keras.html#saving-and-loading-checkpoints" title="Permalink to this headline">#</a></h2>
<p><a class="reference internal" href="../ray-air/api/checkpoint.html#checkpoint-api-ref"><span class="std std-ref">Checkpoints</span></a> can be saved by calling <code class="docutils literal notranslate"><span class="pre">train.report(metrics,</span> <span class="pre">checkpoint=Checkpoint(...))</span></code> in the
training function. This will cause the checkpoint state from the distributed
workers to be saved on the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> (where your python script is executed).</p>
<p>The latest saved checkpoint can be accessed through the <code class="docutils literal notranslate"><span class="pre">checkpoint</span></code> attribute of
the <a class="reference internal" href="../tune/api/doc/ray.air.Result.html#ray.air.Result" title="ray.air.result.Result"><code class="xref py py-class docutils literal notranslate"><span class="pre">Result</span></code></a>, and the best saved checkpoints can be accessed by the <code class="docutils literal notranslate"><span class="pre">best_checkpoints</span></code>
attribute.</p>
<p>Concrete examples are provided to demonstrate how checkpoints (model weights but not models) are saved
appropriately in distributed training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="c1"># data   : X - dim = (n, 4)</span>
    <span class="c1"># target : Y - dim = (n, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
        <span class="c1"># toy neural network : 1-layer</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,))])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
<span class="hll">        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</span>        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="nb">dict</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">model_weights</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
        <span class="p">)</span>
        <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">({},</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
<span class="c1"># {&#39;epoch&#39;: 4, &#39;model_weights&#39;: [array([[-0.31858477],</span>
<span class="c1">#    [ 0.03747174],</span>
<span class="c1">#    [ 0.28266194],</span>
<span class="c1">#    [ 0.8626015 ]], dtype=float32), array([0.02230084], dtype=float32)], &#39;_timestamp&#39;: 1656107383, &#39;_preprocessor&#39;: None, &#39;_current_checkpoint_id&#39;: 4}</span>
</pre></div>
</div>
<p>By default, checkpoints will be persisted to local disk in the <a class="reference internal" href="distributed-pytorch/persistent-storage.html#train-log-dir"><span class="std std-ref">log
directory</span></a> of each run.</p>
<section id="loading-checkpoints">
<h3>Loading checkpoints<a class="headerlink" href="distributed-tensorflow-keras.html#loading-checkpoints" title="Permalink to this headline">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ray</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="nn">ray.air</span> <span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">ScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.train.tensorflow</span> <span class="kn">import</span> <span class="n">TensorflowTrainer</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="c1"># create a toy dataset</span>
    <span class="c1"># data   : X - dim = (n, 4)</span>
    <span class="c1"># target : Y - dim = (n, 1)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="hll">
</span>    <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
        <span class="c1"># toy neural network : 1-layer</span>
<span class="hll">        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,))])</span>
</span><span class="hll">        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">get_checkpoint</span><span class="p">()</span>
</span>        <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
            <span class="c1"># assume that we have run the train.report() example</span>
<span class="hll">            <span class="c1"># and successfully save some model weights</span>
</span><span class="hll">            <span class="n">checkpoint_dict</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
</span><span class="hll">            <span class="n">model</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">checkpoint_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_weights&quot;</span><span class="p">))</span>
</span>            <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">])</span>
<span class="hll">
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">]):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="nb">dict</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">model_weights</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>
        <span class="p">)</span>
        <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">({},</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># save a checkpoint</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># load a checkpoint</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TensorflowTrainer</span><span class="p">(</span>
    <span class="n">train_func</span><span class="p">,</span>
    <span class="n">train_loop_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">},</span>
    <span class="n">scaling_config</span><span class="o">=</span><span class="n">ScalingConfig</span><span class="p">(</span><span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
<span class="c1"># {&#39;epoch&#39;: 4, &#39;model_weights&#39;: [array([[-0.70056134],</span>
<span class="c1">#    [-0.8839263 ],</span>
<span class="c1">#    [-1.0043601 ],</span>
<span class="c1">#    [-0.61634773]], dtype=float32), array([0.01889327], dtype=float32)], &#39;_timestamp&#39;: 1656108446, &#39;_preprocessor&#39;: None, &#39;_current_checkpoint_id&#39;: 3}</span>
</pre></div>
</div>
</section>
</section>
<section id="further-reading">
<h2>Further reading<a class="headerlink" href="distributed-tensorflow-keras.html#further-reading" title="Permalink to this headline">#</a></h2>
<p>We explore more topics in our <a class="reference internal" href="distributed-pytorch.html#train-pytorch-overview"><span class="std std-ref">PyTorch guide</span></a>.
Ray Train is a generic library and the concepts explained there are applicable
to TensorFlow, too.</p>
<p>You may want to look into:</p>
<ul class="simple">
<li><p><a class="reference internal" href="distributed-pytorch/experiment-tracking.html#train-monitoring"><span class="std std-ref">Experiment tracking and callbacks</span></a></p></li>
<li><p><a class="reference internal" href="distributed-pytorch/fault-tolerance.html#train-fault-tolerance"><span class="std std-ref">Fault tolerance and training on spot instances</span></a></p></li>
<li><p><a class="reference internal" href="distributed-pytorch/hyperparameter-optimization.html#train-tune"><span class="std std-ref">Hyperparameter optimization</span></a></p></li>
</ul>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="more-frameworks.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">More Frameworks</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="distributed-xgboost-lightgbm.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Distributed XGBoost and LightGBM</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>Thanks for the feedback!</span>
  </div>
  <div id="csat-inputs">
    <span>Was this helpful?</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>Yes<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>No<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">Feedback</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">Submit</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="https://docs.ray.io/en/master/_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>