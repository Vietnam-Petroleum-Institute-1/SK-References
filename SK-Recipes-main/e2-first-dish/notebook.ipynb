{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Reminder: This üìò `Python` notebook can be run from VS Code with [these prerequisites](../PREREQS.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use this notebook: \n",
    "\n",
    "* Just read the text and scroll along until you run into code blocks.\n",
    "* Code blocks have computer code inside them ‚Äî hover over the block and you can run the code.\n",
    "* Run the code by hitting the ‚ñ∂Ô∏è \"play\" button to the left. If the code runs you'll see a ‚úîÔ∏è. If not, you'll get a ‚ùå.\n",
    "* The output and status of the code block will appear just below itself ‚Äî you need to scroll down further to see it.\n",
    "* Sometimes a code block will ask you for input in a hard-to-notice dialog box üëÜ at the top of your notebook window. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe II: üçΩÔ∏è First Dish\n",
    "## üßë‚Äçüç≥ Let's cook our üßÇüî• first basic dish\n",
    "\n",
    "This notebook has been designed as your \"classroom kitchen\" to get you started quickly with this Semantic Kernel ‚Äî the easy way to add LLM AI to your app. It's in three parts that is best described with technospeak so you get everything just right:\n",
    "\n",
    "1. **Setting up your OpenAI or Azure OpenAI Service key.** This lets you use this notebook like a playground of sorts. And you only have to enter your key once to get going ‚Äî it stores it locally into a file called \"settings.json\" on your disk. üôÄ Be sure to not let that file show up publicly anywhere like on your personal GitHub repo ‚Äî so please .gitignore it.\n",
    "\n",
    "2. **Getting a üî• kernel instantiated.** With your OpenAI or Azure OpenAI key you can then create a kernel to send instructions to. We've made it easy for you to use either OpenAI or Azure OpenAI. When using OpenAI, it will default to your using the `text-davinci-003` model; when you use Azure OpenAI there's an extra endpoint setting to consider ‚Äî and in addition you're asked explicitly for the model you would like to use.\n",
    "\n",
    "3. **Run a semantic üßÇ function.** Okay! You're ready to give your LLM AI a natural language prompt expressed as natural language. We call this kind of interaction with the model \"semantic\" because it lives in the world of the underlying meaning of the text you give to the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1Ô∏è: Set up your OpenAI or Azure OpenAI Service key\n",
    "\n",
    "### 1.1: Make sure you have your API key\n",
    "\n",
    "You will need to make sure you have your OpenAI or Azure OpenAI API keys ready in order to run any of the examples. \n",
    "\n",
    "To do that please copy the `.env.example` file and name it `.env` and put it inside your folder of the notebook that you are running in. That file looks like this: \n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=\"\"\n",
    "OPENAI_ORG_ID=\"\"\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"\"\n",
    "AZURE_OPENAI_ENDPOINT=\"\"\n",
    "AZURE_OPENAI_API_KEY=\"\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's install the Semantic Kernel package from pip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.3.3.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "else:\n",
    "    api_key, org_id = os.environ['OPENAI_API_KEY'], None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above code ran correctly, you are all good to go!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2Ô∏è: Get a üî• kernel ready for you to cook your first dish\n",
    "\n",
    "Congratulations! You're one-third of the way there! Hit ‚ñ∂Ô∏è below to access the locally stored credentials you set up in the first step. This step loads the `semantic-kernel` pip package and gets the rest of the notebook prepared to get your semantic function running asap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, OpenAITextCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureTextCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    # api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAITextCompletion(\"text-davinci-003\", api_key, org_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3Ô∏è: Run a semantic üßÇ function in Semantic Kernel to get üî• cooking\n",
    "\n",
    "Before you set off to write a semantic function, review our documentation on semantic functions at our [learning hub](https://learn.microsoft.com/en-us/semantic-kernel/howto/semanticfunctions). You'll want to feel comfortable with two ideas:\n",
    "\n",
    "* Creating a parameterized prompt with one variable ‚Äî `$input` is the default input variable ‚Äî that you can learn more about [here](https://learn.microsoft.com/semantic-kernel/howto/semanticfunctions#writing-a-more-powerful-templated-prompt).\n",
    "\n",
    "* Configuring your prompt with a few standard settings ‚Äî `max_tokens`, `temperature`, `top_p` ‚Äî that you can learn more about [here](https://learn.microsoft.com/en-us/semantic-kernel/howto/configuringfunctions).\n",
    "\n",
    "You are just three more steps to running a semantic function. Get ready!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Define a parameterized prompt that takes a single input\n",
    "\n",
    "The following code is what we call a \"semantic function\" ‚Äî which is almost equivalent to the word \"prompt.\" You'll hear those terms used interchangeably. In addition you'll hear the phrase \"semantic skill\" ‚Äî which you'll wonder to yourself, \"Is that the same as a semantic function?\" No. But it's also \"almost equivalent.\" This will make sense to you after a few times. No worries and keep on going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A string has been set to be used as a semantic function.\n"
     ]
    }
   ],
   "source": [
    "my_semantic_function_inline = \"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Summarize the content above in less than 140 characters.\n",
    "\"\"\"\n",
    "\n",
    "print(\"A string has been set to be used as a semantic function.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the semantic function we're going to define takes an `$input` text and it will summarize it to less than 140 characters. Sound good? Let's keep going."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Tune the prompt to be more non-deterministic (creative) or deterministic (straight)\n",
    "\n",
    "#### üîµ FAST TRACK Most people will just run the code below without much thought to tuning it, to start\n",
    "\n",
    "The `max_tokens` setting determines how much processing latitude you're giving to the model ‚Äî the smaller it is, the less likely it will be to complete your ask. It's the single most important [setting](https://learn.microsoft.com/en-us/semantic-kernel/howto/configuringfunctions) for you to know because it impacts how much you are spending with each request.\n",
    "\n",
    "Also, you can subtly shape the output of the response with the other two parameters. To make the response more or less \"creative,\" tweak the `temperature` setting between 0 (straight ball) and 1 (curve ball). You can also set the `top_p` setting between 0 (smaller vocabulary) and 1 (larger vocabulary) for a different kind of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A semantic function has been registered.\n"
     ]
    }
   ],
   "source": [
    "summary_function = kernel.create_semantic_function(my_semantic_function_inline,\n",
    "                                                    max_tokens=200,\n",
    "                                                    temperature=0,\n",
    "                                                    top_p=0.5)\n",
    "\n",
    "print(\"A semantic function has been registered.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Set your input to the templated prompt and have the kernel üî• process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023 will be the most exciting year for AI yet, with generative AI creating new works and tools like GPT-3 and DALL-E 2 changing the way we work and play.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sk_input = \"\"\"\n",
    "I think with some confidence I can say that 2023 is going to be the most exciting year that \n",
    "the AI community has ever had,‚Äù writes Kevin Scott, chief technology officer at Microsoft, \n",
    "in a Q&A on the company‚Äôs AI blog. He acknowledges that he also thought 2022 was the most \n",
    "exciting year for AI, but he believes that the pace of innovation is only increasing. \n",
    "This is particularly true with generative AI, which doesn‚Äôt simply analyze large data sets \n",
    "but is a tool people can use to create entirely new works. We can already see its promise \n",
    "in systems like GPT-3, which can do anything from helping copyedit and summarize text to \n",
    "providing inspiration, and DALL-E 2, which can create useful and arresting works of art \n",
    "based on text inputs. Here are some of Scott‚Äôs predictions about how AI will change the \n",
    "way we work and play.\n",
    "\"\"\"\n",
    "# Text source: https://www.microsoft.com/en-us/worklab/kevin-scott-on-5-ways-generative-ai-will-transform-work-in-2023\n",
    "\n",
    "summary = summary_function(sk_input)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You've made your first Semantic Kernel semantic function. Congratulations üî•!\n",
    "\n",
    "> ‚úÖ Be sure to use `text-davinci-003` instead of the more trendy `gpt-3.5-turbo` when you run the above.\n",
    "\n",
    "> ü§î **Get `\"Error: Throttling: Too many requests ...\"` message?** The OpenAI services turn out to be extremely popular these days. If you're using the key for a free account, this message will pop up often. \n",
    "\n",
    "> üò± **Get a different error message?** If you can't see a summarization of the text above, then go to https://aka.ms/sk/discord where we have realtime support available to troubleshoot your problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è≠Ô∏è Next Steps\n",
    "\n",
    "Run through more advanced examples in the notebooks that are available in our GitHub repo at [https://aka.ms/sk/repo](https://aka.ms/sk/repo).\n",
    "\n",
    "[Learn about üßÇ skills!](../e3-skills-rack/notebook.ipynb)\n",
    "\n",
    "Or stay a longer while and change the prompt above to your liking; and also the `$input` and other parameters to your liking. Please keep in mind that each API call to OpenAI or Azure OpenAI Services will use up tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
