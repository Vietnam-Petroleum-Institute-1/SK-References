{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# SK: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest.\n",
    "Pre-requisite: You have already run L4-SK-CreateDB notebook to import the product catalog CSV file to a Chroma vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade semantic-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6ccfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x10f33b6d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import semantic_kernel as sk\n",
    "import os\n",
    "import logging\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger('__name__')\n",
    "kernel=sk.Kernel(log=logger)\n",
    "\n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "kernel.add_chat_service(\n",
    "        \"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo-0301\", api_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26340721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x10f33b6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "kernel.add_text_embedding_generation_service(\n",
    "        \"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a2779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "from qdrant_client import QdrantClient\n",
    "from semantic_kernel.connectors.memory.qdrant import QdrantMemoryStore\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "            url=os.environ['QDRANT_URL'], \n",
    "            api_key=os.environ['QDRANT_API_KEY'],\n",
    "            timeout=20,\n",
    "        )\n",
    "\n",
    "qdrantMemory = QdrantMemoryStore(768)\n",
    "qdrantMemory._qdrantclient = qdrant_client\n",
    "\n",
    "kernel.register_memory_store(memory_store=qdrantMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3988444",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Công đoàn là gì?\"\n",
    "summarize = kernel.create_semantic_function(query, temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd4a10a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'_is_reference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Query the vector DB locally\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m kernel\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msearch_async(collection\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, limit\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_relevance_score\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, query\u001b[39m=\u001b[39mquery)\n",
      "File \u001b[0;32m~/miniforge3/envs/booksage/lib/python3.11/site-packages/semantic_kernel/memory/semantic_text_memory.py:152\u001b[0m, in \u001b[0;36mSemanticTextMemory.search_async\u001b[0;34m(self, collection, query, limit, min_relevance_score, with_embeddings)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Search the memory (calls the memory store's get_nearest_matches method).\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[39mArguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m    List[MemoryQueryResult] -- The list of MemoryQueryResult found.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m query_embedding \u001b[39m=\u001b[39m (\n\u001b[1;32m    150\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embeddings_generator\u001b[39m.\u001b[39mgenerate_embeddings_async([query])\n\u001b[1;32m    151\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 152\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage\u001b[39m.\u001b[39mget_nearest_matches_async(\n\u001b[1;32m    153\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection,\n\u001b[1;32m    154\u001b[0m     embedding\u001b[39m=\u001b[39mquery_embedding,\n\u001b[1;32m    155\u001b[0m     limit\u001b[39m=\u001b[39mlimit,\n\u001b[1;32m    156\u001b[0m     min_relevance_score\u001b[39m=\u001b[39mmin_relevance_score,\n\u001b[1;32m    157\u001b[0m     with_embeddings\u001b[39m=\u001b[39mwith_embeddings,\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    160\u001b[0m \u001b[39mreturn\u001b[39;00m [MemoryQueryResult\u001b[39m.\u001b[39mfrom_memory_record(r[\u001b[39m0\u001b[39m], r[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/miniforge3/envs/booksage/lib/python3.11/site-packages/semantic_kernel/connectors/memory/qdrant/qdrant_memory_store.py:256\u001b[0m, in \u001b[0;36mQdrantMemoryStore.get_nearest_matches_async\u001b[0;34m(self, collection_name, embedding, limit, min_relevance_score, with_embeddings)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mget_nearest_matches_async\u001b[39m(\n\u001b[1;32m    241\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    242\u001b[0m     collection_name: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     with_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    247\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[MemoryRecord, \u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m    248\u001b[0m     match_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qdrantclient\u001b[39m.\u001b[39msearch(\n\u001b[1;32m    249\u001b[0m         collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[1;32m    250\u001b[0m         query_vector\u001b[39m=\u001b[39membedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         with_vectors\u001b[39m=\u001b[39mwith_embeddings,\n\u001b[1;32m    254\u001b[0m     )\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    257\u001b[0m         (\n\u001b[1;32m    258\u001b[0m             MemoryRecord(\n\u001b[1;32m    259\u001b[0m                 is_reference\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mpayload[\u001b[39m\"\u001b[39;49m\u001b[39m_is_reference\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    260\u001b[0m                 external_source_name\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mpayload[\u001b[39m\"\u001b[39;49m\u001b[39m_external_source_name\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    261\u001b[0m                 \u001b[39mid\u001b[39;49m\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mpayload[\u001b[39m\"\u001b[39;49m\u001b[39m_id\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    262\u001b[0m                 description\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mpayload[\u001b[39m\"\u001b[39;49m\u001b[39m_description\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    263\u001b[0m                 text\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mpayload[\u001b[39m\"\u001b[39;49m\u001b[39m_text\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    264\u001b[0m                 additional_metadata\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mpayload[\u001b[39m\"\u001b[39;49m\u001b[39m_additional_metadata\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    265\u001b[0m                 embedding\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mvector,\n\u001b[1;32m    266\u001b[0m                 key\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mid,\n\u001b[1;32m    267\u001b[0m                 timestamp\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mpayload[\u001b[39m\"\u001b[39;49m\u001b[39m_timestamp\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    268\u001b[0m             ),\n\u001b[1;32m    269\u001b[0m             result\u001b[39m.\u001b[39;49mscore,\n\u001b[1;32m    270\u001b[0m         )\n\u001b[1;32m    271\u001b[0m         \u001b[39mfor\u001b[39;49;00m result \u001b[39min\u001b[39;49;00m match_results\n\u001b[1;32m    272\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniforge3/envs/booksage/lib/python3.11/site-packages/semantic_kernel/connectors/memory/qdrant/qdrant_memory_store.py:259\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mget_nearest_matches_async\u001b[39m(\n\u001b[1;32m    241\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    242\u001b[0m     collection_name: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     with_embeddings: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    247\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[MemoryRecord, \u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m    248\u001b[0m     match_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qdrantclient\u001b[39m.\u001b[39msearch(\n\u001b[1;32m    249\u001b[0m         collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[1;32m    250\u001b[0m         query_vector\u001b[39m=\u001b[39membedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         with_vectors\u001b[39m=\u001b[39mwith_embeddings,\n\u001b[1;32m    254\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    257\u001b[0m         (\n\u001b[1;32m    258\u001b[0m             MemoryRecord(\n\u001b[0;32m--> 259\u001b[0m                 is_reference\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39;49mpayload[\u001b[39m\"\u001b[39;49m\u001b[39m_is_reference\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    260\u001b[0m                 external_source_name\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mpayload[\u001b[39m\"\u001b[39m\u001b[39m_external_source_name\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    261\u001b[0m                 \u001b[39mid\u001b[39m\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mpayload[\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    262\u001b[0m                 description\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mpayload[\u001b[39m\"\u001b[39m\u001b[39m_description\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m                 text\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mpayload[\u001b[39m\"\u001b[39m\u001b[39m_text\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    264\u001b[0m                 additional_metadata\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mpayload[\u001b[39m\"\u001b[39m\u001b[39m_additional_metadata\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    265\u001b[0m                 embedding\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mvector,\n\u001b[1;32m    266\u001b[0m                 key\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mid,\n\u001b[1;32m    267\u001b[0m                 timestamp\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mpayload[\u001b[39m\"\u001b[39m\u001b[39m_timestamp\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    268\u001b[0m             ),\n\u001b[1;32m    269\u001b[0m             result\u001b[39m.\u001b[39mscore,\n\u001b[1;32m    270\u001b[0m         )\n\u001b[1;32m    271\u001b[0m         \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m match_results\n\u001b[1;32m    272\u001b[0m     ]\n",
      "\u001b[0;31mKeyError\u001b[0m: '_is_reference'"
     ]
    }
   ],
   "source": [
    "# Query the vector DB locally\n",
    "docs = await kernel.memory.search_async(collection=\"context\", limit=5, min_relevance_score=0.3, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13528f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets augment the LLM query with retrieval from the local vector DB with the RAG (Retrieval Augmented Generation) pattern\n",
    "# The prompt below should be self explanatory of what we are tryign to do with this RAG pattern\n",
    "async def ragqna(kernel, query, limit) -> str:\n",
    "    # Step1: Retrieval: Get list of documents from local DB matching the query\n",
    "    docs = await kernel.memory.search_async(collection=\"outdoordb\", limit=limit, min_relevance_score=0.3, query=query)\n",
    "    # Step2: Augment: Construct the augmented prompt from the retrieved document. Retrieved docs separated by triple backticks to make it easy for LLM to instruct\n",
    "    qdocs = \"\\n```\\n\".join([docs[i].text for i in range(len(docs))])\n",
    "    \n",
    "    prompt = \"\"\"{{ $qdocs}} \n",
    "    \n",
    "    Question: Please query above documents delimited by triple backticks for {{ $query }} \n",
    "    and return results in a table in markdown and summarize each one.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step3: Generation: Generate a summary and markdown formatted output as requested in the prompt from the LLM API\n",
    "    summarize = kernel.create_semantic_function(prompt, temperature=0.0)\n",
    "    context_variables = sk.ContextVariables(variables={\n",
    "        \"qdocs\": qdocs,\n",
    "        \"query\": query\n",
    "    })\n",
    "    response = summarize(variables=context_variables)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await ragqna(kernel, \"shirts with sunblocking\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(str(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21322e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
