{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Reminder: This üìò `Python` notebook can be run from VS Code with [these prerequisites](../PREREQS.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use this notebook: \n",
    "\n",
    "* Just read the text and scroll along until you run into code blocks.\n",
    "* Code blocks have computer code inside them ‚Äî hover over the block and you can run the code.\n",
    "* Run the code by hitting the ‚ñ∂Ô∏è \"play\" button to the left. If the code runs you'll see a ‚úîÔ∏è. If not, you'll get a ‚ùå.\n",
    "* The output and status of the code block will appear just below itself ‚Äî you need to scroll down further to see it.\n",
    "* Sometimes a code block will ask you for input in a hard-to-notice dialog box üëÜ at the top of your notebook window. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Recipe: üïµÔ∏è Test your prompts\n",
    "## üß™ Non-deterministic computation requires testing\n",
    "\n",
    "Because the output from LLM AI is unpredictable in nature, you're going to want to enforce some degree of reliability that it will behave a similar way each time. \n",
    "\n",
    "## Step 1: Instantiate a üî• kernel so we can start cooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.3.3.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, OpenAITextCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", AzureTextCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_text_completion_service(\"dv\", OpenAITextCompletion(\"text-davinci-003\", api_key, org_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: We're going to make one semantic function and run it at two different temperatures\n",
    "\n",
    "Different temperatures result in different outcomes ‚Äî we can illustrate that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "myFun = \"\"\"\n",
    "{{$input}}\n",
    "Define the term above in less than five words.\n",
    "\"\"\"\n",
    "\n",
    "myFunCold = kernel.create_semantic_function(prompt_template=myFun,\n",
    "                skill_name=\"MyTest\", function_name=\"A\", temperature=0.2, top_p=0.1)\n",
    "myFunHot = kernel.create_semantic_function(prompt_template=myFun,\n",
    "                skill_name=\"MyTest\", function_name=\"A\", temperature=1, top_p=1.0)\n",
    "\n",
    "print(\"A semantic function has been registered with two configurations.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run these two functions over multiple iterations to see how they differ, or not. Note that our configurations are as follows:\n",
    "\n",
    "### Configuration A\n",
    "\n",
    "* Temperature = 0.2 <-- ranges from 0 (low randomness) to 1 (high randomness)\n",
    "* TopP = 0.1 <-- ranges from 0 (low variability) to 1 (high variability)\n",
    "\n",
    "### Configuration B\n",
    "\n",
    "* Temperature = 1 <-- ranges from 0 (low randomness) to 1 (high randomness)\n",
    "* TopP = 1.0 <-- ranges from 0 (low variability) to 1 (high variability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "skInput = \"AI\"\n",
    "coldResponses = \"\"\n",
    "hotResponses = \"\"\n",
    "for i in range(5):\n",
    "   coldResult = myFunCold(skInput)\n",
    "   coldResponses += str(coldResult).strip() + \"\\n\"\n",
    "   hotResult = myFunHot(skInput)\n",
    "   hotResponses += str(hotResult).strip() + \"\\n\"\n",
    "\n",
    "print(f\"COLD responses:\\n\\n{coldResponses}\")\n",
    "print(f\"\\nHOT responses:\\n\\n{hotResponses}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Let's get freezing cold to aim towards a higher degree of determinism in outputs.\n",
    "\n",
    "You should note that the COLD responses don't deviate so much; whereas the HOT responses do. Another set of parameters to consider that increase variability are `PresencePenalty` and `FrequencyPenalty`. As the official definition goes:\n",
    "\n",
    "* `PresencePenalty`: \"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\"\n",
    "* `FrequencyPenalty`: \"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\"\n",
    "\n",
    "So if you have the LLM AI generating a longer piece of text, you can either have it try to use new words (instead of repeating old ones) either by nudging the usage frequency or count of the word (i.e. \"presence\") with positive number settings. Or when using negative number settings it will be happy repeating itself by using the same words over and over. \n",
    "\n",
    "The benefit of using the same words repetitively is that the LLM AI won't veer off course so easily. That said, you may think it's a bit ... boring. But sometimes boring is what you want when you'd like the same answer to always get generated. Let's try this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "myFunNext = \"\"\"\n",
    "Term: {{$input}}\n",
    "A fifteen word story about the term:\n",
    "\"\"\"\n",
    "\n",
    "myFunColder = kernel.create_semantic_function(prompt_template=myFunNext,\n",
    "                skill_name=\"MyTest\", function_name=\"C\", temperature=0.2,\n",
    "                top_p=0.1, presence_penalty=2, frequency_penalty=2)\n",
    "myFunHotter = kernel.create_semantic_function(prompt_template=myFunNext,\n",
    "                skill_name=\"MyTest\", function_name=\"D\", temperature=1,\n",
    "                presence_penalty=-2, frequency_penalty=-2, top_p=1.0)\n",
    "\n",
    "print(\"A semantic function has been registered with two more EXTREME configurations.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's run them to see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "skInput = \"AI\"\n",
    "colderResponses = \"\"\n",
    "hotterResponses = \"\"\n",
    "\n",
    "for i in range(3):\n",
    "   colderResult = myFunColder(skInput)\n",
    "   colderResponses += str(colderResult).strip() + \"\\n\"\n",
    "   hotterResult = myFunHotter(skInput)\n",
    "   hotterResponses += str(hotterResult).strip() + \"\\n\"\n",
    "\n",
    "print(f\"COLDER responses:\\n\\n{colderResponses}\")\n",
    "print(f\"\\nHOTTER responses:\\n\\n{hotterResponses}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you understand how to \"tamp down\" the LLM AI's degree of creativity with a COLDER approach. And also how to unleash its freedom of expression with a HOTTER approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Just one more thing ‚Äî let's use a different model\n",
    "\n",
    "Changing the model will also change the result. Let's see that up close by comparing what the output of `text-davinci-003` will be compared with `text-davinci-002` (the older model). We'll start by making a second kernel that uses `text-davinci-002` called `kernel2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, OpenAITextCompletion\n",
    "\n",
    "kernel2 = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel2.add_text_completion_service(\"dv\", AzureTextCompletion(deployment, endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel2.add_text_completion_service(\"dv\", OpenAITextCompletion(\"text-davinci-002\", api_key, org_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll run the same HOTTER vs COLDER scenario with `text-davinci-002` in charge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "myFunColder2 = kernel2.create_semantic_function(prompt_template=myFunNext,\n",
    "                skill_name=\"MyTest\", function_name=\"C2\", temperature=0.2,\n",
    "                top_p=0.1, presence_penalty=2, frequency_penalty=2)\n",
    "myFunHotter2 = kernel2.create_semantic_function(prompt_template=myFunNext,\n",
    "                skill_name=\"MyTest\", function_name=\"D2\", temperature=1,\n",
    "                presence_penalty=-2, frequency_penalty=-2, top_p=1.0)\n",
    "\n",
    "colderResponses2 = \"\"\n",
    "hotterResponses2 = \"\"\n",
    "\n",
    "for i in range(3):\n",
    "   colderResult2 = myFunColder2(skInput)\n",
    "   colderResponses2 += str(colderResult2).strip() + \"\\n\"\n",
    "   hotterResult2 = myFunHotter(skInput)\n",
    "   hotterResponses2 += str(hotterResult2).strip() + \"\\n\"\n",
    "\n",
    "print(f\"COLDER responses (text-davinci-002):\\n\\n{colderResponses2}\")\n",
    "print(f\"\\nHOTTER responses (text-davinci-002):\\n\\n{hotterResponses2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look carefully, the COLDER responses should be similar or the same. The HOTTER ones, however, will be different. You'll notice that `text-davinci-003` felt less robot-like compared with `text-davinci-002.` But don't forget that each model has different cost structures -- so you can choose the model that matches your quality needs and economics by learning how to tune these basic parameters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è≠Ô∏è Next Steps\n",
    "\n",
    "Run through more advanced examples in the notebooks that are available in our GitHub repo at [https://aka.ms/sk/repo](https://aka.ms/sk/repo).\n",
    "\n",
    "[You're all done! Visit the main GitHub repo to check out what's new ‚Äî because LLM AI is changing ever-so-rapidly!](https://aka.ms/sk/repo)\n",
    "\n",
    "Or, stay a longer while and modify the various parameters to get a better feel from them. Just be careful how many times you keep calling the models in a loop ‚Äî because the $$$ can quickly add up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
