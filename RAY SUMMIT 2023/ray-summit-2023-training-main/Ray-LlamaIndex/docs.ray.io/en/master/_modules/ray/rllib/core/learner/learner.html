
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.core.learner.learner &#8212; Ray 3.0.0.dev0</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="https://docs.ray.io/en/master/_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="https://docs.ray.io/en/master/_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="https://docs.ray.io/en/master/_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="https://docs.ray.io/en/master/_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="https://docs.ray.io/en/master/_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/pygments.css" />
    <link rel="stylesheet" href="https://docs.ray.io/en/master/_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/css/termynal.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/_/static/css/badge_only.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="https://docs.ray.io/en/master/_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../../" id="documentation_options" src="https://docs.ray.io/en/master/_static/documentation_options.js"></script>
    <script src="https://docs.ray.io/en/master/_static/jquery.js"></script>
    <script src="https://docs.ray.io/en/master/_static/underscore.js"></script>
    <script src="https://docs.ray.io/en/master/_static/doctools.js"></script>
    <script src="https://docs.ray.io/en/master/_static/clipboard.min.js"></script>
    <script src="https://docs.ray.io/en/master/_static/copybutton.js"></script>
    <script src="https://docs.ray.io/en/master/_static/js/versionwarning.js"></script>
    <script src="https://docs.ray.io/en/master/_static/togglebutton.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/docsearch.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/csat.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/termynal.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/custom.js"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/top-navigation.js"></script>
    <script src="https://docs.ray.io/en/master/_static/js/tags.js"></script>
    <script src="https://docs.ray.io/en/master/_static/tabs.js"></script>
    <script src="https://docs.ray.io/en/master/_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://docs.ray.io/en/master/_static/design-tabs.js"></script>
    <script async="async" src="https://docs.ray.io/_/static/javascript/readthedocs-doc-embed.js"></script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/core/learner/learner.html" />
    <link rel="shortcut icon" href="https://docs.ray.io/en/master/_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110413294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110413294-1');
</script>

<script
  src="https://widget.kapa.ai/kapa-widget.bundle.js"
  data-website-id="18a8c339-4ec5-43c8-8182-db3f2bc8c6b6"
  data-project-name="Ray"
  data-project-color="#2C2C2C"
  data-project-logo="https://global.discourse-cdn.com/business7/uploads/ray/original/1X/8f4dcb72f7cd34e2a332d548bd65860994bc8ff1.png"
></script>

<script>
(function(apiKey){
    (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=o._q||[];
    v=['initialize','identify','updateOptions','pageLoad','track'];for(w=0,x=v.length;w<x;++w)(function(m){
        o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
        y=e.createElement(n);y.async=!0;y.src='https://cdn.pendo.io/agent/static/'+apiKey+'/pendo.js';
        z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

        pendo.initialize({
            visitor: {
                id: 'VISITOR-UNIQUE-ID'
            },
            account: {
                id: 'ACCOUNT-UNIQUE-ID'
            }
        });
})('f89fa48a-6dd7-4d7c-67cf-a8051ed891f2');
</script>



  
<!-- RTD Extra Head -->

<link rel="stylesheet" href="https://docs.ray.io/_/static/css/readthedocs-doc-embed.css" type="text/css" />

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": false, "api_host": "https://readthedocs.com", "builder": "sphinx", "canonical_url": null, "docroot": "/doc/source/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-2", "language": "en", "page": "_modules/ray/rllib/core/learner/learner", "programming_language": "py", "project": "anyscale-ray", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_book_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="https://docs.ray.io/_/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"><div class='topnav'></div></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Ray 3.0.0.dev0</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main Navigation">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../../index.html">
                    Welcome to Ray!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Ray
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/index.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-more-libs/installation.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/use-cases.html">
   Use Cases
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/examples.html">
   Example Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-overview/ray-libraries.html">
   Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-core/walkthrough.html">
   Ray Core
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../data/data.html">
   Ray Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../train/train.html">
   Ray Train
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../tune.html">
   Ray Tune
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../serve/index.html">
   Ray Serve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../rllib/index.html">
   Ray RLlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-more-libs/index.html">
   More Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-core/cluster/index.html">
   Ray Clusters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-observability/index.html">
   Monitoring and Debugging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-references/api.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../../ray-contribute/stability.html">
   Developer Guides
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ray-project/ray"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ray-project/ray/issues/new?title=Issue%20on%20page%20%2F_modules/ray/rllib/core/learner/learner.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <h1>Source code for ray.rllib.core.learner.learner</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">abc</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">DefaultDict</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Hashable</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.learner.reduce_result_dict_fn</span> <span class="kn">import</span> <span class="n">_reduce_mean_results</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.learner.scaling_config</span> <span class="kn">import</span> <span class="n">LearnerGroupScalingConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.marl_module</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MultiAgentRLModule</span><span class="p">,</span>
    <span class="n">MultiAgentRLModuleSpec</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.rl_module</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">RLModule</span><span class="p">,</span>
    <span class="n">ModuleID</span><span class="p">,</span>
    <span class="n">SingleAgentRLModuleSpec</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span> <span class="n">MultiAgentBatch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OverrideToImplementCustomLogic</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic_CallToSuperRecommended</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.debug</span> <span class="kn">import</span> <span class="n">update_global_seed_if_necessary</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.framework</span> <span class="kn">import</span> <span class="n">try_import_tf</span><span class="p">,</span> <span class="n">try_import_torch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ALL_MODULES</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.minibatch_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MiniBatchDummyIterator</span><span class="p">,</span>
    <span class="n">MiniBatchCyclicIterator</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.nested_dict</span> <span class="kn">import</span> <span class="n">NestedDict</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.numpy</span> <span class="kn">import</span> <span class="n">convert_to_numpy</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.schedules.scheduler</span> <span class="kn">import</span> <span class="n">Scheduler</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.serialization</span> <span class="kn">import</span> <span class="n">serialize_type</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">LearningRateOrSchedule</span><span class="p">,</span>
    <span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">Param</span><span class="p">,</span>
    <span class="n">ParamRef</span><span class="p">,</span>
    <span class="n">ParamDict</span><span class="p">,</span>
    <span class="n">ResultDict</span><span class="p">,</span>
    <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.torch.torch_compile_config</span> <span class="kn">import</span> <span class="n">TorchCompileConfig</span>

<span class="n">torch</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">try_import_torch</span><span class="p">()</span>
<span class="n">tf1</span><span class="p">,</span> <span class="n">tf</span><span class="p">,</span> <span class="n">tfv</span> <span class="o">=</span> <span class="n">try_import_tf</span><span class="p">()</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">DEFAULT_OPTIMIZER</span> <span class="o">=</span> <span class="s2">&quot;default_optimizer&quot;</span>

<span class="c1"># COMMON LEARNER LOSS_KEYS</span>
<span class="n">POLICY_LOSS_KEY</span> <span class="o">=</span> <span class="s2">&quot;policy_loss&quot;</span>
<span class="n">VF_LOSS_KEY</span> <span class="o">=</span> <span class="s2">&quot;vf_loss&quot;</span>
<span class="n">ENTROPY_KEY</span> <span class="o">=</span> <span class="s2">&quot;entropy&quot;</span>

<span class="c1"># Additional update keys</span>
<span class="n">LEARNER_RESULTS_CURR_LR_KEY</span> <span class="o">=</span> <span class="s2">&quot;curr_lr&quot;</span>


<div class="viewcode-block" id="TorchCompileWhatToCompile"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.TorchCompileWhatToCompile.html#ray.rllib.core.learner.learner.TorchCompileWhatToCompile">[docs]</a><span class="k">class</span> <span class="nc">TorchCompileWhatToCompile</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">Enum</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enumerates schemes of what parts of the TorchLearner can be compiled.</span>

<span class="sd">    This can be either the entire update step of the learner or only the forward</span>
<span class="sd">    methods (and therein the forward_train method) of the RLModule.</span>

<span class="sd">    .. note::</span>
<span class="sd">        - torch.compiled code can become slow on graph breaks or even raise</span>
<span class="sd">            errors on unsupported operations. Empirically, compiling</span>
<span class="sd">            `forward_train` should introduce little graph breaks, raise no</span>
<span class="sd">            errors but result in a speedup comparable to compiling the</span>
<span class="sd">            complete update.</span>
<span class="sd">        - Using `complete_update` is experimental and may result in errors.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Compile the entire update step of the learner.</span>
    <span class="c1"># This includes the forward pass of the RLModule, the loss computation, and the</span>
    <span class="c1"># optimizer step.</span>
    <span class="n">COMPLETE_UPDATE</span> <span class="o">=</span> <span class="s2">&quot;complete_update&quot;</span>
    <span class="c1"># Only compile the forward methods (and therein the forward_train method) of the</span>
    <span class="c1"># RLModule.</span>
    <span class="n">FORWARD_TRAIN</span> <span class="o">=</span> <span class="s2">&quot;forward_train&quot;</span></div>


<div class="viewcode-block" id="FrameworkHyperparameters"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.FrameworkHyperparameters.html#ray.rllib.core.learner.learner.FrameworkHyperparameters">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">FrameworkHyperparameters</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;The framework specific hyper-parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        eager_tracing: Whether to trace the model in eager mode. This enables tf</span>
<span class="sd">            tracing mode by wrapping the loss function computation in a tf.function.</span>
<span class="sd">            This is useful for speeding up the training loop. However, it is not</span>
<span class="sd">            compatible with all tf operations. For example, tf.print is not supported</span>
<span class="sd">            in tf.function.</span>
<span class="sd">        torch_compile: Whether to use torch.compile() within the context of a given</span>
<span class="sd">            learner.</span>
<span class="sd">        what_to_compile: What to compile when using torch.compile(). Can be one of</span>
<span class="sd">            [TorchCompileWhatToCompile.complete_update,</span>
<span class="sd">            TorchCompileWhatToCompile.forward_train].</span>
<span class="sd">            If `complete_update`, the update step of the learner will be compiled. This</span>
<span class="sd">            includes the forward pass of the RLModule, the loss computation, and the</span>
<span class="sd">            optimizer step.</span>
<span class="sd">            If `forward_train`, only the forward methods (and therein the</span>
<span class="sd">            forward_train method) of the RLModule will be compiled.</span>
<span class="sd">            Either of the two may lead to different performance gains in different</span>
<span class="sd">            settings.</span>
<span class="sd">            `complete_update` promises the highest performance gains, but may not work</span>
<span class="sd">            in some settings. By compiling only forward_train, you may already get</span>
<span class="sd">            some speedups and avoid issues that arise from compiling the entire update.</span>
<span class="sd">        troch_compile_config: The TorchCompileConfig to use for compiling the RL</span>
<span class="sd">            Module in Torch.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">eager_tracing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch_compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">what_to_compile</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">TorchCompileWhatToCompile</span><span class="o">.</span><span class="n">FORWARD_TRAIN</span>
    <span class="n">torch_compile_cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TorchCompileConfig&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">what_to_compile</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="n">TorchCompileWhatToCompile</span><span class="o">.</span><span class="n">FORWARD_TRAIN</span><span class="p">,</span>
                <span class="n">TorchCompileWhatToCompile</span><span class="o">.</span><span class="n">COMPLETE_UPDATE</span><span class="p">,</span>
            <span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;what_to_compile must be one of [&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;TorchCompileWhatToCompile.forward_train, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;TorchCompileWhatToCompile.complete_update] but is&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">what_to_compile</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">torch_compile_cfg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;torch_compile_cfg must be set when torch_compile is True.&quot;</span>
                <span class="p">)</span></div>


<div class="viewcode-block" id="LearnerHyperparameters"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.LearnerHyperparameters.html#ray.rllib.core.learner.learner.LearnerHyperparameters">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LearnerHyperparameters</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Hyperparameters for a Learner, derived from a subset of AlgorithmConfig values.</span>

<span class="sd">    Instances of this class should only be created via calling</span>
<span class="sd">    `get_learner_hyperparameters()` on a frozen AlgorithmConfig object and should always</span>
<span class="sd">    considered read-only.</span>

<span class="sd">    When creating a new Learner, you should also define a new sub-class of this class</span>
<span class="sd">    and make sure the respective AlgorithmConfig sub-class has a proper implementation</span>
<span class="sd">    of the `get_learner_hyperparameters` method.</span>

<span class="sd">    Validation of the values of these hyperparameters should be done by the</span>
<span class="sd">    respective AlgorithmConfig class.</span>

<span class="sd">    For configuring different learning behaviors for different (single-agent) RLModules</span>
<span class="sd">    within the Learner, RLlib uses the `_per_module_overrides` property (dict), mapping</span>
<span class="sd">    ModuleID to a overridden version of self, in which the module-specific override</span>
<span class="sd">    settings are applied.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Parameters used for gradient postprocessing (clipping) and gradient application.</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="n">LearningRateOrSchedule</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">grad_clip_by</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Maps ModuleIDs to LearnerHyperparameters that are to be used for that particular</span>
    <span class="c1"># module.</span>
    <span class="c1"># You can access the module-specific `LearnerHyperparameters` object for a given</span>
    <span class="c1"># module_id by using the `get_hps_for_module(module_id=..)` API.</span>
    <span class="n">_per_module_overrides</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">,</span> <span class="s2">&quot;LearnerHyperparameters&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="LearnerHyperparameters.get_hps_for_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.LearnerHyperparameters.get_hps_for_module.html#ray.rllib.core.learner.learner.LearnerHyperparameters.get_hps_for_module">[docs]</a>    <span class="k">def</span> <span class="nf">get_hps_for_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LearnerHyperparameters&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns a LearnerHyperparameter instance, given a `module_id`.</span>

<span class="sd">        This is useful for passing these module-specific HPs to a Learner&#39;s</span>
<span class="sd">        `..._for_module(module_id=.., hps=..)` methods. Individual modules within</span>
<span class="sd">        a MultiAgentRLModule can then override certain AlgorithmConfig settings</span>
<span class="sd">        of the main config, e.g. the learning rate.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The module ID for which to return a specific</span>
<span class="sd">                LearnerHyperparameter instance.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The module specific LearnerHyperparameter instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># ModuleID found in our overrides dict. Return module specific HPs.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_per_module_overrides</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_module_overrides</span>
        <span class="p">):</span>
            <span class="c1"># In case, the per-module sub-HPs object is still a dict, convert</span>
            <span class="c1"># it to a fully qualified LearnerHyperparameter object here first.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_per_module_overrides</span><span class="p">[</span><span class="n">module_id</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_per_module_overrides</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)(</span>
                    <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_per_module_overrides</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="c1"># Return the module specific version of self.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_module_overrides</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>
        <span class="c1"># ModuleID not found in overrides or the overrides dict is None</span>
        <span class="c1"># -&gt; return self.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span></div></div>


<div class="viewcode-block" id="Learner"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.html#ray.rllib.core.learner.learner.Learner">[docs]</a><span class="k">class</span> <span class="nc">Learner</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Base class for Learners.</span>

<span class="sd">    This class will be used to train RLModules. It is responsible for defining the loss</span>
<span class="sd">    function, and updating the neural network weights that it owns. It also provides a</span>
<span class="sd">    way to add/remove modules to/from RLModules in a multi-agent scenario, in the</span>
<span class="sd">    middle of training (This is useful for league based training).</span>

<span class="sd">    TF and Torch specific implementation of this class fills in the framework-specific</span>
<span class="sd">    implementation details for distributed training, and for computing and applying</span>
<span class="sd">    gradients. User should not need to sub-class this class, but instead inherit from</span>
<span class="sd">    the TF or Torch specific sub-classes to implement their algorithm-specific update</span>
<span class="sd">    logic.</span>

<span class="sd">    Args:</span>
<span class="sd">        module_spec: The module specification for the RLModule that is being trained.</span>
<span class="sd">            If the module is a single agent module, after building the module it will</span>
<span class="sd">            be converted to a multi-agent module with a default key. Can be none if the</span>
<span class="sd">            module is provided directly via the `module` argument. Refer to</span>
<span class="sd">            ray.rllib.core.rl_module.SingleAgentRLModuleSpec</span>
<span class="sd">            or ray.rllib.core.rl_module.MultiAgentRLModuleSpec for more info.</span>
<span class="sd">        module: If learner is being used stand-alone, the RLModule can be optionally</span>
<span class="sd">            passed in directly instead of the through the `module_spec`.</span>
<span class="sd">        scaling_config: Configuration for scaling the learner actors.</span>
<span class="sd">            Refer to ray.rllib.core.learner.scaling_config.LearnerGroupScalingConfig</span>
<span class="sd">            for more info.</span>
<span class="sd">        learner_hyperparameters: The hyper-parameters for the Learner.</span>
<span class="sd">            Algorithm specific learner hyper-parameters will passed in via this</span>
<span class="sd">            argument. For example in PPO the `vf_loss_coeff` hyper-parameter will be</span>
<span class="sd">            passed in via this argument. Refer to</span>
<span class="sd">            ray.rllib.core.learner.learner.LearnerHyperparameters for more info.</span>
<span class="sd">        framework_hps: The framework specific hyper-parameters. This will be used to</span>
<span class="sd">            pass in any framework specific hyper-parameter that will impact the module</span>
<span class="sd">            creation. For example `eager_tracing` in TF or `torch.compile()` in Torch.</span>
<span class="sd">            Refer to ray.rllib.core.learner.learner.FrameworkHyperparameters for</span>
<span class="sd">            more info.</span>


<span class="sd">    Usage pattern:</span>

<span class="sd">        Note: We use PPO and torch as an example here because many of the showcased</span>
<span class="sd">        components need implementations to come together. However, the same</span>
<span class="sd">        pattern is generally applicable.</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (</span>
<span class="sd">                PPOTorchRLModule</span>
<span class="sd">            )</span>
<span class="sd">            from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog</span>
<span class="sd">            from ray.rllib.core.learner.torch.torch_learner import TorchLearner</span>
<span class="sd">            import gymnasium as gym</span>

<span class="sd">            env = gym.make(&quot;CartPole-v1&quot;)</span>

<span class="sd">            # Create a single agent RL module spec.</span>
<span class="sd">            module_spec = SingleAgentRLModuleSpec(</span>
<span class="sd">                module_class=PPOTorchRLModule,</span>
<span class="sd">                observation_space=env.observation_space,</span>
<span class="sd">                action_space=env.action_space,</span>
<span class="sd">                model_config_dict = {&quot;hidden&quot;: [128, 128]},</span>
<span class="sd">                catalog_class = PPOCatalog,</span>
<span class="sd">            )</span>

<span class="sd">            # Create a learner instance that will train the module</span>
<span class="sd">            learner = TorchLearner(module_spec=module_spec)</span>

<span class="sd">            # Note: the learner should be built before it can be used.</span>
<span class="sd">            learner.build()</span>

<span class="sd">            # Take one gradient update on the module and report the results</span>
<span class="sd">            # results = learner.update(...)</span>

<span class="sd">            # Add a new module, perhaps for league based training</span>
<span class="sd">            learner.add_module(</span>
<span class="sd">                module_id=&quot;new_player&quot;,</span>
<span class="sd">                module_spec=SingleAgentRLModuleSpec(</span>
<span class="sd">                    module_class=PPOTorchRLModule,</span>
<span class="sd">                    observation_space=env.observation_space,</span>
<span class="sd">                    action_space=env.action_space,</span>
<span class="sd">                    model_config_dict = {&quot;hidden&quot;: [128, 128]},</span>
<span class="sd">                    catalog_class = PPOCatalog,</span>
<span class="sd">                )</span>
<span class="sd">            )</span>

<span class="sd">            # Take another gradient update with both previous and new modules.</span>
<span class="sd">            # results = learner.update(...)</span>

<span class="sd">            # Remove a module</span>
<span class="sd">            learner.remove_module(&quot;new_player&quot;)</span>

<span class="sd">            # Will train previous modules only.</span>
<span class="sd">            # results = learner.update(...)</span>

<span class="sd">            # Get the state of the learner</span>
<span class="sd">            state = learner.get_state()</span>

<span class="sd">            # Set the state of the learner</span>
<span class="sd">            learner.set_state(state)</span>

<span class="sd">            # Get the weights of the underly multi-agent RLModule</span>
<span class="sd">            weights = learner.get_module_state()</span>

<span class="sd">            # Set the weights of the underly multi-agent RLModule</span>
<span class="sd">            learner.set_module_state(weights)</span>


<span class="sd">    Extension pattern:</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            from ray.rllib.core.learner.torch.torch_learner import TorchLearner</span>

<span class="sd">            class MyLearner(TorchLearner):</span>

<span class="sd">               def compute_loss(self, fwd_out, batch):</span>
<span class="sd">                   # compute the loss based on batch and output of the forward pass</span>
<span class="sd">                   # to access the learner hyper-parameters use `self._hps`</span>
<span class="sd">                   return {ALL_MODULES: loss}</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">framework</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">TOTAL_LOSS_KEY</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;total_loss&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">SingleAgentRLModuleSpec</span><span class="p">,</span> <span class="n">MultiAgentRLModuleSpec</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RLModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learner_group_scaling_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LearnerGroupScalingConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learner_hyperparameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LearnerHyperparameters</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">framework_hyperparameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FrameworkHyperparameters</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># We first set seeds</span>
        <span class="k">if</span> <span class="n">learner_hyperparameters</span> <span class="ow">and</span> <span class="n">learner_hyperparameters</span><span class="o">.</span><span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">update_global_seed_if_necessary</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">framework</span><span class="p">,</span> <span class="n">learner_hyperparameters</span><span class="o">.</span><span class="n">seed</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="p">(</span><span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Exactly one of `module_spec` or `module` must be provided to Learner!&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_module_spec</span> <span class="o">=</span> <span class="n">module_spec</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_obj</span> <span class="o">=</span> <span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hps</span> <span class="o">=</span> <span class="n">learner_hyperparameters</span> <span class="ow">or</span> <span class="n">LearnerHyperparameters</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># pick the configs that we need for the learner from scaling config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_learner_group_scaling_config</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">learner_group_scaling_config</span> <span class="ow">or</span> <span class="n">LearnerGroupScalingConfig</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_distributed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learner_group_scaling_config</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_gpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learner_group_scaling_config</span><span class="o">.</span><span class="n">num_gpus_per_worker</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="c1"># if we are using gpu but we are not distributed, use this gpu for training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_local_gpu_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learner_group_scaling_config</span><span class="o">.</span><span class="n">local_gpu_idx</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_framework_hyperparameters</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">framework_hyperparameters</span> <span class="ow">or</span> <span class="n">FrameworkHyperparameters</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_framework_hyperparameters</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>

        <span class="c1"># whether self.build has already been called</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_built</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># These are the attributes that are set during build.</span>

        <span class="c1"># The actual MARLModule used by this Learner.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentRLModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># These are set for properly applying optimizers and adding or removing modules.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">ParamRef</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_named_optimizers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">:</span> <span class="n">ParamDict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Dict mapping ModuleID to a list of optimizer names. Note that the optimizer</span>
        <span class="c1"># name includes the ModuleID as a prefix: optimizer_name=`[ModuleID]_[.. rest]`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_optimizers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

        <span class="c1"># Only manage optimizer&#39;s learning rate if user has NOT overridden</span>
        <span class="c1"># the `configure_optimizers_for_module` method. Otherwise, leave responsibility</span>
        <span class="c1"># to handle lr-updates entirely in user&#39;s hands.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_lr_schedules</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">Scheduler</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Registered metrics (one sub-dict per module ID) to be returned from</span>
        <span class="c1"># `Learner.update()`. These metrics will be &quot;compiled&quot; automatically into</span>
        <span class="c1"># the final results dict in the `self.compile_update_results()` method.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">distributed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Whether the learner is running in distributed mode.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distributed</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentRLModule</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The multi-agent RLModule that is being trained.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">hps</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LearnerHyperparameters</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The hyper-parameters for the learner.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hps</span>

<div class="viewcode-block" id="Learner.register_optimizer"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.register_optimizer.html#ray.rllib.core.learner.learner.Learner.register_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">register_optimizer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span> <span class="o">=</span> <span class="n">ALL_MODULES</span><span class="p">,</span>
        <span class="n">optimizer_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_OPTIMIZER</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Param</span><span class="p">],</span>
        <span class="n">lr_or_lr_schedule</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LearningRateOrSchedule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Registers an optimizer with a ModuleID, name, param list and lr-scheduler.</span>

<span class="sd">        Use this method in your custom implementations of either</span>
<span class="sd">        `self.configure_optimizers()` or `self.configure_optimzers_for_module()` (you</span>
<span class="sd">        should only override one of these!). If you register a learning rate Scheduler</span>
<span class="sd">        setting together with an optimizer, RLlib will automatically keep this</span>
<span class="sd">        optimizer&#39;s learning rate updated throughout the training process.</span>
<span class="sd">        Alternatively, you can construct your optimizers directly with a learning rate</span>
<span class="sd">        and manage learning rate scheduling or updating yourself.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The `module_id` under which to register the optimizer. If not</span>
<span class="sd">                provided, will assume ALL_MODULES.</span>
<span class="sd">            optimizer_name: The name (str) of the optimizer. If not provided, will</span>
<span class="sd">                assume DEFAULT_OPTIMIZER.</span>
<span class="sd">            optimizer: The already instantiated optimizer object to register.</span>
<span class="sd">            params: A list of parameters (framework-specific variables) that will be</span>
<span class="sd">                trained/updated</span>
<span class="sd">            lr_or_lr_schedule: An optional fixed learning rate or learning rate schedule</span>
<span class="sd">                setup. If provided, RLlib will automatically keep the optimizer&#39;s</span>
<span class="sd">                learning rate updated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Validate optimizer instance and its param list.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_registered_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

        <span class="n">full_registration_name</span> <span class="o">=</span> <span class="n">module_id</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">optimizer_name</span>

        <span class="c1"># Store the given optimizer under the given `module_id`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_optimizers</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">full_registration_name</span><span class="p">)</span>

        <span class="c1"># Store the optimizer instance under its full `module_id`_`optimizer_name`</span>
        <span class="c1"># key.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_named_optimizers</span><span class="p">[</span><span class="n">full_registration_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="c1"># Store all given parameters under the given optimizer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_parameters</span><span class="p">[</span><span class="n">optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">param_ref</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_param_ref</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_parameters</span><span class="p">[</span><span class="n">optimizer</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param_ref</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">param_ref</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>

        <span class="c1"># Optionally, store a scheduler object along with this optimizer. If such a</span>
        <span class="c1"># setting is provided, RLlib will handle updating the optimizer&#39;s learning rate</span>
        <span class="c1"># over time.</span>
        <span class="k">if</span> <span class="n">lr_or_lr_schedule</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Validate the given setting.</span>
            <span class="n">Scheduler</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span>
                <span class="n">fixed_value_or_schedule</span><span class="o">=</span><span class="n">lr_or_lr_schedule</span><span class="p">,</span>
                <span class="n">setting_name</span><span class="o">=</span><span class="s2">&quot;lr_or_lr_schedule&quot;</span><span class="p">,</span>
                <span class="n">description</span><span class="o">=</span><span class="s2">&quot;learning rate or schedule&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Create the scheduler object for this optimizer.</span>
            <span class="n">scheduler</span> <span class="o">=</span> <span class="n">Scheduler</span><span class="p">(</span>
                <span class="n">fixed_value_or_schedule</span><span class="o">=</span><span class="n">lr_or_lr_schedule</span><span class="p">,</span>
                <span class="n">framework</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">framework</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_lr_schedules</span><span class="p">[</span><span class="n">optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="n">scheduler</span>
            <span class="c1"># Set the optimizer to the current (first) learning rate.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_optimizer_lr</span><span class="p">(</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_current_value</span><span class="p">(),</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="Learner.configure_optimizers"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.configure_optimizers.html#ray.rllib.core.learner.learner.Learner.configure_optimizers">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Configures, creates, and registers the optimizers for this Learner.</span>

<span class="sd">        Optimizers are responsible for updating the model&#39;s parameters during training,</span>
<span class="sd">        based on the computed gradients.</span>

<span class="sd">        Normally, you should not override this method for your custom algorithms</span>
<span class="sd">        (which require certain optimizers), but rather override the</span>
<span class="sd">        `self.configure_optimizers_for_module(module_id=..)` method and register those</span>
<span class="sd">        optimizers in there that you need for the given `module_id`.</span>

<span class="sd">        You can register an optimizer for any RLModule within `self.module` (or for</span>
<span class="sd">        the ALL_MODULES ID) by calling `self.register_optimizer()` and passing the</span>
<span class="sd">        module_id, optimizer_name (only in case you would like to register more than</span>
<span class="sd">        one optimizer for a given module), the optimizer instane itself, a list</span>
<span class="sd">        of all the optimizer&#39;s parameters (to be updated by the optimizer), and</span>
<span class="sd">        an optional learning rate or learning rate schedule setting.</span>

<span class="sd">        This method is called once during building (`self.build()`).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># The default implementation simply calls `self.configure_optimizers_for_module`</span>
        <span class="c1"># on each RLModule within `self.module`.</span>
        <span class="k">for</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_module_compatible_with_learner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">[</span><span class="n">module_id</span><span class="p">]):</span>
                <span class="n">hps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">get_hps_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">configure_optimizers_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span> <span class="n">hps</span><span class="o">=</span><span class="n">hps</span><span class="p">)</span></div>

<div class="viewcode-block" id="Learner.configure_optimizers_for_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.configure_optimizers_for_module.html#ray.rllib.core.learner.learner.Learner.configure_optimizers_for_module">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">configure_optimizers_for_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span> <span class="n">hps</span><span class="p">:</span> <span class="n">LearnerHyperparameters</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Configures an optimizer for the given module_id.</span>

<span class="sd">        This method is called for each RLModule in the Multi-Agent RLModule being</span>
<span class="sd">        trained by the Learner, as well as any new module added during training via</span>
<span class="sd">        `self.add_module()`. It should configure and construct one or more optimizers</span>
<span class="sd">        and register them via calls to `self.register_optimizer()` along with the</span>
<span class="sd">        `module_id`, an optional optimizer name (str), a list of the optimizer&#39;s</span>
<span class="sd">        framework specific parameters (variables), and an optional learning rate value</span>
<span class="sd">        or -schedule.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The module_id of the RLModule that is being configured.</span>
<span class="sd">            hps: The LearnerHyperparameters specific to the given `module_id`.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner.compute_gradients"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.compute_gradients.html#ray.rllib.core.learner.learner.Learner.compute_gradients">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">compute_gradients</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">loss_per_module</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParamDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Computes the gradients based on the given losses.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss_per_module: Dict mapping module IDs to their individual total loss</span>
<span class="sd">                terms, computed by the individual `compute_loss_for_module()` calls.</span>
<span class="sd">                The overall total loss (sum of loss terms over all modules) is stored</span>
<span class="sd">                under `loss_per_module[ALL_MODULES]`.</span>
<span class="sd">            **kwargs: Forward compatibility kwargs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The gradients in the same (flat) format as self._params. Note that all</span>
<span class="sd">            top-level structures, such as module IDs, will not be present anymore in</span>
<span class="sd">            the returned dict. It will merely map parameter tensor references to their</span>
<span class="sd">            respective gradient tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner.postprocess_gradients"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.postprocess_gradients.html#ray.rllib.core.learner.learner.Learner.postprocess_gradients">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">postprocess_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients_dict</span><span class="p">:</span> <span class="n">ParamDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParamDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Applies potential postprocessing operations on the gradients.</span>

<span class="sd">        This method is called after gradients have been computed and modifies them</span>
<span class="sd">        before they are applied to the respective module(s) by the optimizer(s).</span>
<span class="sd">        This might include grad clipping by value, norm, or global-norm, or other</span>
<span class="sd">        algorithm specific gradient postprocessing steps.</span>

<span class="sd">        This default implementation calls `self.postprocess_gradients_for_module()`</span>
<span class="sd">        on each of the sub-modules in our MultiAgentRLModule: `self.module` and</span>
<span class="sd">        returns the accumulated gradients dicts.</span>

<span class="sd">        Args:</span>
<span class="sd">            gradients_dict: A dictionary of gradients in the same (flat) format as</span>
<span class="sd">                self._params. Note that top-level structures, such as module IDs,</span>
<span class="sd">                will not be present anymore in this dict. It will merely map gradient</span>
<span class="sd">                tensor references to gradient tensors.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary with the updated gradients and the exact same (flat) structure</span>
<span class="sd">            as the incoming `gradients_dict` arg.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># The flat gradients dict (mapping param refs to params), returned by this</span>
        <span class="c1"># method.</span>
        <span class="n">postprocessed_gradients</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="c1"># Send a gradients dict for only this `module_id` to the</span>
            <span class="c1"># `self.postprocess_gradients_for_module()` method.</span>
            <span class="n">module_grads_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_optimizers_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">):</span>
                <span class="n">module_grads_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">filter_param_dict_for_optimizer</span><span class="p">(</span><span class="n">gradients_dict</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="n">module_grads_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocess_gradients_for_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
                <span class="n">hps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">get_hps_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">),</span>
                <span class="n">module_gradients_dict</span><span class="o">=</span><span class="n">module_grads_dict</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module_grads_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>

            <span class="c1"># Update our return dict.</span>
            <span class="n">postprocessed_gradients</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">module_grads_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">postprocessed_gradients</span></div>

<div class="viewcode-block" id="Learner.postprocess_gradients_for_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.postprocess_gradients_for_module.html#ray.rllib.core.learner.learner.Learner.postprocess_gradients_for_module">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">postprocess_gradients_for_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span>
        <span class="n">hps</span><span class="p">:</span> <span class="n">LearnerHyperparameters</span><span class="p">,</span>
        <span class="n">module_gradients_dict</span><span class="p">:</span> <span class="n">ParamDict</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParamDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Applies postprocessing operations on the gradients of the given module.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The module ID for which we will postprocess computed gradients.</span>
<span class="sd">                Note that `module_gradients_dict` already only carries those gradient</span>
<span class="sd">                tensors that belong to this `module_id`. Other `module_id`&#39;s gradients</span>
<span class="sd">                are not available in this call.</span>
<span class="sd">            hps: The LearnerHyperparameters specific to the given `module_id`.</span>
<span class="sd">            module_gradients_dict: A dictionary of gradients in the same (flat) format</span>
<span class="sd">                as self._params, mapping gradient refs to gradient tensors, which are to</span>
<span class="sd">                be postprocessed. You may alter these tensors in place or create new</span>
<span class="sd">                ones and return these in a new dict.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary with the updated gradients and the exact same (flat) structure</span>
<span class="sd">            as the incoming `module_gradients_dict` arg.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">postprocessed_grads</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">hps</span><span class="o">.</span><span class="n">grad_clip</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">postprocessed_grads</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">module_gradients_dict</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">postprocessed_grads</span>

        <span class="k">for</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_optimizers_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">):</span>
            <span class="n">grad_dict_to_clip</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">filter_param_dict_for_optimizer</span><span class="p">(</span>
                <span class="n">param_dict</span><span class="o">=</span><span class="n">module_gradients_dict</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Perform gradient clipping, if configured.</span>
            <span class="n">global_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_clip_function</span><span class="p">()(</span>
                <span class="n">grad_dict_to_clip</span><span class="p">,</span>
                <span class="n">grad_clip</span><span class="o">=</span><span class="n">hps</span><span class="o">.</span><span class="n">grad_clip</span><span class="p">,</span>
                <span class="n">grad_clip_by</span><span class="o">=</span><span class="n">hps</span><span class="o">.</span><span class="n">grad_clip_by</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">hps</span><span class="o">.</span><span class="n">grad_clip_by</span> <span class="o">==</span> <span class="s2">&quot;global_norm&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_metric</span><span class="p">(</span>
                    <span class="n">module_id</span><span class="p">,</span>
                    <span class="sa">f</span><span class="s2">&quot;gradients_</span><span class="si">{</span><span class="n">optimizer_name</span><span class="si">}</span><span class="s2">_global_norm&quot;</span><span class="p">,</span>
                    <span class="n">global_norm</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">postprocessed_grads</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">grad_dict_to_clip</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">postprocessed_grads</span></div>

<div class="viewcode-block" id="Learner.apply_gradients"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.apply_gradients.html#ray.rllib.core.learner.learner.Learner.apply_gradients">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">apply_gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients_dict</span><span class="p">:</span> <span class="n">ParamDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Applies the gradients to the MultiAgentRLModule parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            gradients_dict: A dictionary of gradients in the same (flat) format as</span>
<span class="sd">                self._params. Note that top-level structures, such as module IDs,</span>
<span class="sd">                will not be present anymore in this dict. It will merely map gradient</span>
<span class="sd">                tensor references to gradient tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner.register_metric"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.register_metric.html#ray.rllib.core.learner.learner.Learner.register_metric">[docs]</a>    <span class="k">def</span> <span class="nf">register_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Registers a single key/value metric pair for loss- and gradient stats.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The module_id to register the metric under. This may be</span>
<span class="sd">                ALL_MODULES.</span>
<span class="sd">            key: The name of the metric to register (below the given `module_id`).</span>
<span class="sd">            value: The actual value of the metric. This might also be a tensor var (e.g.</span>
<span class="sd">                from within a traced tf2 function).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="p">[</span><span class="n">module_id</span><span class="p">][</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span></div>

<div class="viewcode-block" id="Learner.register_metrics"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.register_metrics.html#ray.rllib.core.learner.learner.Learner.register_metrics">[docs]</a>    <span class="k">def</span> <span class="nf">register_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">metrics_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Registers several key/value metric pairs for loss- and gradient stats.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The module_id to register the metrics under. This may be</span>
<span class="sd">                ALL_MODULES.</span>
<span class="sd">            metrics_dict: A dict mapping names of metrics to be registered (below the</span>
<span class="sd">                given `module_id`) to the actual values of these metrics. Values might</span>
<span class="sd">                also be tensor vars (e.g. from within a traced tf2 function).</span>
<span class="sd">                These will be automatically converted to numpy values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_metric</span><span class="p">(</span><span class="n">module_id</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Learner.get_optimizer"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.get_optimizer.html#ray.rllib.core.learner.learner.Learner.get_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">optimizer_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DEFAULT_OPTIMIZER</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optimizer</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the optimizer object, configured under the given module_id and name.</span>

<span class="sd">        If only one optimizer was registered under `module_id` (or ALL_MODULES)</span>
<span class="sd">        via the `self.register_optimizer` method, `optimizer_name` is assumed to be</span>
<span class="sd">        DEFAULT_OPTIMIZER.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The ModuleID for which to return the configured optimizer.</span>
<span class="sd">                If not provided, will assume DEFAULT_POLICY_ID.</span>
<span class="sd">            optimizer_name: The name of the optimizer (registered under `module_id` via</span>
<span class="sd">                `self.register_optimizer()`) to return. If not provided, will assume</span>
<span class="sd">                DEFAULT_OPTIMIZER.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The optimizer object, configured under the given `module_id` and</span>
<span class="sd">            `optimizer_name`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">full_registration_name</span> <span class="o">=</span> <span class="n">module_id</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">optimizer_name</span>
        <span class="k">assert</span> <span class="n">full_registration_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_optimizers</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_optimizers</span><span class="p">[</span><span class="n">full_registration_name</span><span class="p">]</span></div>

<div class="viewcode-block" id="Learner.get_optimizers_for_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.get_optimizers_for_module.html#ray.rllib.core.learner.learner.Learner.get_optimizers_for_module">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizers_for_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span> <span class="o">=</span> <span class="n">ALL_MODULES</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Returns a list of (optimizer_name, optimizer instance)-tuples for module_id.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The ModuleID for which to return the configured</span>
<span class="sd">                (optimizer name, optimizer)-pairs. If not provided, will return</span>
<span class="sd">                optimizers registered under ALL_MODULES.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of tuples of the format: ([optimizer_name], [optimizer object]),</span>
<span class="sd">            where optimizer_name is the name under which the optimizer was registered</span>
<span class="sd">            in `self.register_optimizer`. If only a single optimizer was</span>
<span class="sd">            configured for `module_id`, [optimizer_name] will be DEFAULT_OPTIMIZER.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">named_optimizers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">full_registration_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_optimizers</span><span class="p">[</span><span class="n">module_id</span><span class="p">]:</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_optimizers</span><span class="p">[</span><span class="n">full_registration_name</span><span class="p">]</span>
            <span class="c1"># TODO (sven): How can we avoid registering optimziers under this</span>
            <span class="c1">#  constructed `[module_id]_[optim_name]` format?</span>
            <span class="n">optim_name</span> <span class="o">=</span> <span class="n">full_registration_name</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">module_id</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
            <span class="n">named_optimizers</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">optim_name</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">named_optimizers</span></div>

<div class="viewcode-block" id="Learner.filter_param_dict_for_optimizer"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.filter_param_dict_for_optimizer.html#ray.rllib.core.learner.learner.Learner.filter_param_dict_for_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">filter_param_dict_for_optimizer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">:</span> <span class="n">ParamDict</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ParamDict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Reduces the given ParamDict to contain only parameters for given optimizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            param_dict: The ParamDict to reduce/filter down to the given `optimizer`.</span>
<span class="sd">                The returned dict will be a subset of `param_dict` only containing keys</span>
<span class="sd">                (param refs) that were registered together with `optimizer` (and thus</span>
<span class="sd">                that `optimizer` is responsible for applying gradients to).</span>
<span class="sd">            optimizer: The optimizer object to whose parameter refs the given</span>
<span class="sd">                `param_dict` should be reduced.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A new ParamDict only containing param ref keys that belong to `optimizer`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Return a sub-dict only containing those param_ref keys (and their values)</span>
        <span class="c1"># that belong to the `optimizer`.</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">ref</span><span class="p">:</span> <span class="n">param_dict</span><span class="p">[</span><span class="n">ref</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">ref</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_parameters</span><span class="p">[</span><span class="n">optimizer</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">ref</span> <span class="ow">in</span> <span class="n">param_dict</span> <span class="ow">and</span> <span class="n">param_dict</span><span class="p">[</span><span class="n">ref</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">}</span></div>

<div class="viewcode-block" id="Learner.get_module_state"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.get_module_state.html#ray.rllib.core.learner.learner.Learner.get_module_state">[docs]</a>    <span class="k">def</span> <span class="nf">get_module_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the state of the underlying MultiAgentRLModule.</span>

<span class="sd">        The output should be numpy-friendly for easy serialization, not framework</span>
<span class="sd">        specific tensors.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_ids: The ids of the modules to get the weights for. If None, all</span>
<span class="sd">                modules will be returned.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary that holds the state of the modules in a numpy-friendly</span>
<span class="sd">            format.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">module_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">module_ids</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">convert_to_numpy</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">module_states</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span></div>

<div class="viewcode-block" id="Learner.set_module_state"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.set_module_state.html#ray.rllib.core.learner.learner.Learner.set_module_state">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">set_module_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the state of the underlying MultiAgentRLModule&quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner.get_param_ref"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.get_param_ref.html#ray.rllib.core.learner.learner.Learner.get_param_ref">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">get_param_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Param</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Hashable</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns a hashable reference to a trainable parameter.</span>

<span class="sd">        This should be overriden in framework specific specialization. For example in</span>
<span class="sd">        torch it will return the parameter itself, while in tf it returns the .ref() of</span>
<span class="sd">        the variable. The purpose is to retrieve a unique reference to the parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            param: The parameter to get the reference to.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A reference to the parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner.get_parameters"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.get_parameters.html#ray.rllib.core.learner.learner.Learner.get_parameters">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">get_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">RLModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Param</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the list of parameters of a module.</span>

<span class="sd">        This should be overriden in framework specific learner. For example in torch it</span>
<span class="sd">        will return .parameters(), while in tf it returns .trainable_variables.</span>

<span class="sd">        Args:</span>
<span class="sd">            module: The module to get the parameters from.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The parameters of the module.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner._convert_batch_type"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._convert_batch_type.html#ray.rllib.core.learner.learner.Learner._convert_batch_type">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_convert_batch_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">MultiAgentBatch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentBatch</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Converts the elements of a MultiAgentBatch to Tensors on the correct device.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The MultiAgentBatch object to convert.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The resulting MultiAgentBatch with framework-specific tensor values placed</span>
<span class="sd">            on the correct device.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner.compile_results"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.compile_results.html#ray.rllib.core.learner.learner.Learner.compile_results">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">compile_results</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">MultiAgentBatch</span><span class="p">,</span>
        <span class="n">fwd_out</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">loss_per_module</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">],</span>
        <span class="n">metrics_per_module</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Compile results from the update in a numpy-friendly format.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The batch that was used for the update.</span>
<span class="sd">            fwd_out: The output of the forward train pass.</span>
<span class="sd">            loss_per_module: A dict mapping module IDs (including ALL_MODULES) to the</span>
<span class="sd">                individual loss tensors as returned by calls to</span>
<span class="sd">                `compute_loss_for_module(module_id=...)`.</span>
<span class="sd">            metrics_per_module: The collected metrics defaultdict mapping ModuleIDs to</span>
<span class="sd">                metrics dicts. These metrics are collected during loss- and</span>
<span class="sd">                gradient computation, gradient postprocessing, and gradient application.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of results sub-dicts per module (including ALL_MODULES).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">MultiAgentBatch</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;batch must be a MultiAgentBatch, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># We compile the metrics to have the structure:</span>
        <span class="c1"># top-leve key: module_id -&gt; [key, e.g. self.TOTAL_LOSS_KEY] -&gt; [value].</span>
        <span class="c1"># Results will include all registered metrics under the respective module ID</span>
        <span class="c1"># top-level key.</span>
        <span class="n">module_learner_stats</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
        <span class="c1"># Add the num agent|env steps trained counts for all modules.</span>
        <span class="n">module_learner_stats</span><span class="p">[</span><span class="n">ALL_MODULES</span><span class="p">][</span><span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
        <span class="n">module_learner_stats</span><span class="p">[</span><span class="n">ALL_MODULES</span><span class="p">][</span><span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>

        <span class="n">loss_per_module_numpy</span> <span class="o">=</span> <span class="n">convert_to_numpy</span><span class="p">(</span><span class="n">loss_per_module</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">policy_batches</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">ALL_MODULES</span><span class="p">]:</span>
            <span class="c1"># Report total loss per module and other registered metrics.</span>
            <span class="n">module_learner_stats</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">TOTAL_LOSS_KEY</span><span class="p">:</span> <span class="n">loss_per_module_numpy</span><span class="p">[</span><span class="n">module_id</span><span class="p">],</span>
                    <span class="o">**</span><span class="n">convert_to_numpy</span><span class="p">(</span><span class="n">metrics_per_module</span><span class="p">[</span><span class="n">module_id</span><span class="p">]),</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="c1"># Report registered optimizers&#39; learning rates.</span>
            <span class="n">module_learner_stats</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">optim_name</span><span class="si">}</span><span class="s2">_lr&quot;</span><span class="p">:</span> <span class="n">convert_to_numpy</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_get_optimizer_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">optim_name</span><span class="p">,</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">get_optimizers_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">}</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">module_learner_stats</span><span class="p">)</span></div>

<div class="viewcode-block" id="Learner.add_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.add_module.html#ray.rllib.core.learner.learner.Learner.add_module">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">SingleAgentRLModuleSpec</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Add a module to the underlying MultiAgentRLModule and the Learner.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The id of the module to add.</span>
<span class="sd">            module_spec: The module spec of the module to add.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_is_built</span><span class="p">()</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>

        <span class="c1"># Allow the user to configure one or more optimizers for this new module.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">configure_optimizers_for_module</span><span class="p">(</span>
            <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
            <span class="n">hps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">get_hps_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">),</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Learner.remove_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.remove_module.html#ray.rllib.core.learner.learner.Learner.remove_module">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">remove_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Remove a module from the Learner.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The id of the module to remove.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_is_built</span><span class="p">()</span>
        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_module_compatible_with_learner</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="c1"># Delete the removed module&#39;s parameters.</span>
            <span class="n">parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
                <span class="n">param_ref</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_param_ref</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">param_ref</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">:</span>
                    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params</span><span class="p">[</span><span class="n">param_ref</span><span class="p">]</span>
            <span class="c1"># Delete the removed module&#39;s registered optimizers.</span>
            <span class="k">for</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_optimizers_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">):</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_parameters</span><span class="p">[</span><span class="n">optimizer</span><span class="p">]</span>
                <span class="n">name</span> <span class="o">=</span> <span class="n">module_id</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">optimizer_name</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_named_optimizers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_lr_schedules</span><span class="p">:</span>
                    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_lr_schedules</span><span class="p">[</span><span class="n">optimizer</span><span class="p">]</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_optimizers</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">remove_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">)</span></div>

<div class="viewcode-block" id="Learner.build"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.build.html#ray.rllib.core.learner.learner.Learner.build">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Builds the Learner.</span>

<span class="sd">        This method should be called before the learner is used. It is responsible for</span>
<span class="sd">        setting up the RLModule, optimizers, and (optionally) their lr-schedulers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_built</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Learner already built. Skipping build.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_built</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Build the module to be trained by this learner.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_module</span><span class="p">()</span>

        <span class="c1"># Configure, construct, and register all optimizers needed to train</span>
        <span class="c1"># `self.module`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">configure_optimizers</span><span class="p">()</span></div>

<div class="viewcode-block" id="Learner.compute_loss"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.compute_loss.html#ray.rllib.core.learner.learner.Learner.compute_loss">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">fwd_out</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentBatch</span><span class="p">,</span> <span class="n">NestedDict</span><span class="p">],</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">MultiAgentBatch</span><span class="p">,</span> <span class="n">NestedDict</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">TensorType</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Computes the loss for the module being optimized.</span>

<span class="sd">        This method must be overridden by multiagent-specific algorithm learners to</span>
<span class="sd">        specify the specific loss computation logic. If the algorithm is single agent</span>
<span class="sd">        `compute_loss_for_module()` should be overridden instead.</span>
<span class="sd">        `fwd_out` is the output of the `forward_train()` method of the underlying</span>
<span class="sd">        MultiAgentRLModule. `batch` is the data that was used to compute `fwd_out`.</span>
<span class="sd">        The returned dictionary must contain a key called</span>
<span class="sd">        ALL_MODULES, which will be used to compute gradients. It is recommended</span>
<span class="sd">        to not compute any forward passes within this method, and to use the</span>
<span class="sd">        `forward_train()` outputs of the RLModule(s) to compute the required tensors for</span>
<span class="sd">        loss calculations.</span>

<span class="sd">        Args:</span>
<span class="sd">            fwd_out: Output from a call to the `forward_train()` method of self.module</span>
<span class="sd">                during training (`self.update()`).</span>
<span class="sd">            batch: The training batch that was used to compute `fwd_out`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary mapping module IDs to individual loss terms. The dictionary</span>
<span class="sd">            must contain one protected key ALL_MODULES which will be used for computing</span>
<span class="sd">            gradients through.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss_total</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">loss_per_module</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="n">fwd_out</span><span class="p">:</span>
            <span class="n">module_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>
            <span class="n">module_fwd_out</span> <span class="o">=</span> <span class="n">fwd_out</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss_for_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
                <span class="n">hps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">get_hps_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">),</span>
                <span class="n">batch</span><span class="o">=</span><span class="n">module_batch</span><span class="p">,</span>
                <span class="n">fwd_out</span><span class="o">=</span><span class="n">module_fwd_out</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">loss_per_module</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span>

            <span class="k">if</span> <span class="n">loss_total</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">loss_total</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss_total</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="n">loss_per_module</span><span class="p">[</span><span class="n">ALL_MODULES</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_total</span>

        <span class="k">return</span> <span class="n">loss_per_module</span></div>

<div class="viewcode-block" id="Learner.compute_loss_for_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.compute_loss_for_module.html#ray.rllib.core.learner.learner.Learner.compute_loss_for_module">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">compute_loss_for_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span>
        <span class="n">hps</span><span class="p">:</span> <span class="n">LearnerHyperparameters</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">NestedDict</span><span class="p">,</span>
        <span class="n">fwd_out</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Computes the loss for a single module.</span>

<span class="sd">        Think of this as computing loss for a single agent. For multi-agent use-cases</span>
<span class="sd">        that require more complicated computation for loss, consider overriding the</span>
<span class="sd">        `compute_loss` method instead.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The id of the module.</span>
<span class="sd">            hps: The LearnerHyperparameters specific to the given `module_id`.</span>
<span class="sd">            batch: The sample batch for this particular module.</span>
<span class="sd">            fwd_out: The output of the forward pass for this particular module.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A single total loss tensor. If you have more than one optimizer on the</span>
<span class="sd">            provided `module_id` and would like to compute gradients separately using</span>
<span class="sd">            these different optimizers, simply add up the individual loss terms for</span>
<span class="sd">            each optimizer and return the sum. Also, for tracking the individual loss</span>
<span class="sd">            terms, you can use the `Learner.register_metric(s)` APIs.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner.additional_update"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.additional_update.html#ray.rllib.core.learner.learner.Learner.additional_update">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">additional_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module_ids_to_update</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="n">ModuleID</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Apply additional non-gradient based updates to this Algorithm.</span>

<span class="sd">        For example, this could be used to do a polyak averaging update</span>
<span class="sd">        of a target network in off policy algorithms like SAC or DQN.</span>

<span class="sd">        Example:</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            from ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module import (</span>
<span class="sd">                PPOTorchRLModule</span>
<span class="sd">            )</span>
<span class="sd">            from ray.rllib.algorithms.ppo.ppo_catalog import PPOCatalog</span>
<span class="sd">            from ray.rllib.algorithms.ppo.torch.ppo_torch_learner import (</span>
<span class="sd">                PPOTorchLearner</span>
<span class="sd">            )</span>
<span class="sd">            from ray.rllib.algorithms.ppo.ppo_learner import (</span>
<span class="sd">                LEARNER_RESULTS_CURR_KL_COEFF_KEY</span>
<span class="sd">            )</span>
<span class="sd">            from ray.rllib.algorithms.ppo.ppo_learner import PPOLearnerHyperparameters</span>
<span class="sd">            import gymnasium as gym</span>

<span class="sd">            env = gym.make(&quot;CartPole-v1&quot;)</span>
<span class="sd">            hps = PPOLearnerHyperparameters(</span>
<span class="sd">                use_kl_loss=True,</span>
<span class="sd">                kl_coeff=0.2,</span>
<span class="sd">                kl_target=0.01,</span>
<span class="sd">                use_critic=True,</span>
<span class="sd">                clip_param=0.3,</span>
<span class="sd">                vf_clip_param=10.0,</span>
<span class="sd">                entropy_coeff=0.01,</span>
<span class="sd">                entropy_coeff_schedule = [</span>
<span class="sd">                    [0, 0.01],</span>
<span class="sd">                    [20000000, 0.0],</span>
<span class="sd">                ],</span>
<span class="sd">                vf_loss_coeff=0.5,</span>
<span class="sd">            )</span>

<span class="sd">            # Create a single agent RL module spec.</span>
<span class="sd">            module_spec = SingleAgentRLModuleSpec(</span>
<span class="sd">                module_class=PPOTorchRLModule,</span>
<span class="sd">                observation_space=env.observation_space,</span>
<span class="sd">                action_space=env.action_space,</span>
<span class="sd">                model_config_dict = {&quot;hidden&quot;: [128, 128]},</span>
<span class="sd">                catalog_class = PPOCatalog,</span>
<span class="sd">            )</span>

<span class="sd">            class CustomPPOLearner(PPOTorchLearner):</span>
<span class="sd">                def additional_update_for_module(</span>
<span class="sd">                    self, *, module_id, hps, timestep, sampled_kl_values</span>
<span class="sd">                ):</span>

<span class="sd">                    results = super().additional_update_for_module(</span>
<span class="sd">                        module_id=module_id,</span>
<span class="sd">                        hps=hps,</span>
<span class="sd">                        timestep=timestep,</span>
<span class="sd">                        sampled_kl_values=sampled_kl_values,</span>
<span class="sd">                    )</span>

<span class="sd">                    # Try something else than the PPO paper here.</span>
<span class="sd">                    sampled_kl = sampled_kl_values[module_id]</span>
<span class="sd">                    curr_var = self.curr_kl_coeffs_per_module[module_id]</span>
<span class="sd">                    if sampled_kl &gt; 1.2 * self.hps.kl_target:</span>
<span class="sd">                        curr_var.data *= 1.2</span>
<span class="sd">                    elif sampled_kl &lt; 0.8 * self.hps.kl_target:</span>
<span class="sd">                        curr_var.data *= 0.4</span>
<span class="sd">                    results.update({LEARNER_RESULTS_CURR_KL_COEFF_KEY: curr_var.item()})</span>


<span class="sd">            learner = CustomPPOLearner(</span>
<span class="sd">                module_spec=module_spec,</span>
<span class="sd">                learner_hyperparameters=hps,</span>
<span class="sd">            )</span>

<span class="sd">            # Note: the learner should be built before it can be used.</span>
<span class="sd">            learner.build()</span>

<span class="sd">            # Inside a training loop, we can now call the additional update as we like:</span>
<span class="sd">            for i in range(100):</span>
<span class="sd">                # sample = ...</span>
<span class="sd">                # learner.update(sample)</span>
<span class="sd">                if i % 10 == 0:</span>
<span class="sd">                    learner.additional_update(</span>
<span class="sd">                        timestep=i,</span>
<span class="sd">                        sampled_kl_values={&quot;default_policy&quot;: 0.5}</span>
<span class="sd">                    )</span>

<span class="sd">        Args:</span>
<span class="sd">            module_ids_to_update: The ids of the modules to update. If None, all</span>
<span class="sd">                modules will be updated.</span>
<span class="sd">            timestep: The current timestep.</span>
<span class="sd">            **kwargs: Keyword arguments to use for the additional update.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of results from the update</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">results_all_modules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">module_ids</span> <span class="o">=</span> <span class="n">module_ids_to_update</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">module_id</span> <span class="ow">in</span> <span class="n">module_ids</span><span class="p">:</span>
            <span class="n">module_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_update_for_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
                <span class="n">hps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hps</span><span class="o">.</span><span class="n">get_hps_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">),</span>
                <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">results_all_modules</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_results</span>

        <span class="k">return</span> <span class="n">results_all_modules</span></div>

<div class="viewcode-block" id="Learner.additional_update_for_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.additional_update_for_module.html#ray.rllib.core.learner.learner.Learner.additional_update_for_module">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="k">def</span> <span class="nf">additional_update_for_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span>
        <span class="n">hps</span><span class="p">:</span> <span class="n">LearnerHyperparameters</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Apply additional non-gradient based updates for a single module.</span>

<span class="sd">        See `additional_update` for more details.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: The id of the module to update.</span>
<span class="sd">            hps: The LearnerHyperparameters specific to the given `module_id`.</span>
<span class="sd">            timestep: The current global timestep (to be used with schedulers).</span>
<span class="sd">            **kwargs: Keyword arguments to use for the additional update.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of results from the update</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Only cover the optimizer mapped to this particular module.</span>
        <span class="k">for</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_optimizers_for_module</span><span class="p">(</span><span class="n">module_id</span><span class="p">):</span>
            <span class="c1"># Only update this optimizer&#39;s lr, if a scheduler has been registered</span>
            <span class="c1"># along with it.</span>
            <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_lr_schedules</span><span class="p">:</span>
                <span class="n">new_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_lr_schedules</span><span class="p">[</span><span class="n">optimizer</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_optimizer_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">new_lr</span><span class="p">)</span>
                <span class="c1"># Make sure our returned results differentiate by optimizer name</span>
                <span class="c1"># (if not the default name).</span>
                <span class="n">stats_name</span> <span class="o">=</span> <span class="n">LEARNER_RESULTS_CURR_LR_KEY</span>
                <span class="k">if</span> <span class="n">optimizer_name</span> <span class="o">!=</span> <span class="n">DEFAULT_OPTIMIZER</span><span class="p">:</span>
                    <span class="n">stats_name</span> <span class="o">+=</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">optimizer_name</span>
                <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">stats_name</span><span class="p">:</span> <span class="n">new_lr</span><span class="p">})</span>

        <span class="k">return</span> <span class="n">results</span></div>

<div class="viewcode-block" id="Learner.update"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.update.html#ray.rllib.core.learner.learner.Learner.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">MultiAgentBatch</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">minibatch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">reduce_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">List</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]],</span> <span class="n">ResultDict</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">_reduce_mean_results</span>
        <span class="p">),</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;Do `num_iters` minibatch updates given the original batch.</span>

<span class="sd">        Given a batch of episodes you can use this method to take more</span>
<span class="sd">        than one backward pass on the batch. The same minibatch_size and num_iters</span>
<span class="sd">        will be used for all module ids in MultiAgentRLModule.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: A batch of data.</span>
<span class="sd">            minibatch_size: The size of the minibatch to use for each update.</span>
<span class="sd">            num_iters: The number of complete passes over all the sub-batches</span>
<span class="sd">                in the input multi-agent batch.</span>
<span class="sd">            reduce_fn: reduce_fn: A function to reduce the results from a list of</span>
<span class="sd">                minibatch updates. This can be any arbitrary function that takes a</span>
<span class="sd">                list of dictionaries and returns a single dictionary. For example you</span>
<span class="sd">                can either take an average (default) or concatenate the results (for</span>
<span class="sd">                example for metrics) or be more selective about you want to report back</span>
<span class="sd">                to the algorithm&#39;s training_step. If None is passed, the results will</span>
<span class="sd">                not get reduced.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of results, in numpy format or a list of such dictionaries in</span>
<span class="sd">            case `reduce_fn` is None and we have more than one minibatch pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_is_built</span><span class="p">()</span>

        <span class="n">missing_module_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">policy_batches</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_module_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Batch contains module ids that are not in the learner: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">missing_module_ids</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">num_iters</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># We must do at least one pass on the batch for training.</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`num_iters` must be &gt;= 1&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">minibatch_size</span><span class="p">:</span>
            <span class="n">batch_iter</span> <span class="o">=</span> <span class="n">MiniBatchCyclicIterator</span>
        <span class="k">elif</span> <span class="n">num_iters</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># `minibatch_size` was not set but `num_iters` &gt; 1.</span>
            <span class="c1"># Under the old training stack, users could do multiple sgd passes</span>
            <span class="c1"># over a batch without specifying a minibatch size. We enable</span>
            <span class="c1"># this behavior here by setting the minibatch size to be the size</span>
            <span class="c1"># of the batch (e.g. 1 minibatch of size batch.count)</span>
            <span class="n">minibatch_size</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">count</span>
            <span class="n">batch_iter</span> <span class="o">=</span> <span class="n">MiniBatchCyclicIterator</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># `minibatch_size` and `num_iters` are not set by the user.</span>
            <span class="n">batch_iter</span> <span class="o">=</span> <span class="n">MiniBatchDummyIterator</span>

        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Convert input batch into a tensor batch (MultiAgentBatch) on the correct</span>
        <span class="c1"># device (e.g. GPU). We move the batch already here to avoid having to move</span>
        <span class="c1"># every single minibatch that is created in the `batch_iter` below.</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_batch_type</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_slicing_by_batch_id</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">tensor_minibatch</span> <span class="ow">in</span> <span class="n">batch_iter</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">):</span>
            <span class="c1"># Make the actual in-graph/traced `_update` call. This should return</span>
            <span class="c1"># all tensor values (no numpy).</span>
            <span class="n">nested_tensor_minibatch</span> <span class="o">=</span> <span class="n">NestedDict</span><span class="p">(</span><span class="n">tensor_minibatch</span><span class="o">.</span><span class="n">policy_batches</span><span class="p">)</span>
            <span class="p">(</span>
                <span class="n">fwd_out</span><span class="p">,</span>
                <span class="n">loss_per_module</span><span class="p">,</span>
                <span class="n">metrics_per_module</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update</span><span class="p">(</span><span class="n">nested_tensor_minibatch</span><span class="p">)</span>

            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compile_results</span><span class="p">(</span>
                <span class="n">batch</span><span class="o">=</span><span class="n">tensor_minibatch</span><span class="p">,</span>
                <span class="n">fwd_out</span><span class="o">=</span><span class="n">fwd_out</span><span class="p">,</span>
                <span class="n">loss_per_module</span><span class="o">=</span><span class="n">loss_per_module</span><span class="p">,</span>
                <span class="n">metrics_per_module</span><span class="o">=</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="o">**</span><span class="n">metrics_per_module</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_result</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
            <span class="c1"># TODO (sven): Figure out whether `compile_metrics` should be forced</span>
            <span class="c1">#  to return all numpy/python data, then we can skip this conversion</span>
            <span class="c1">#  step here.</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">convert_to_numpy</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>

        <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_slicing_by_batch_id</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Reduce results across all minibatches, if necessary.</span>

        <span class="c1"># If we only have one result anyways, then the user will not expect a list</span>
        <span class="c1"># to be reduced here (and might not provide a `reduce_fn` therefore) -&gt;</span>
        <span class="c1"># Return single results dict.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># If no `reduce_fn` provided, return list of results dicts.</span>
        <span class="k">elif</span> <span class="n">reduce_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">results</span>
        <span class="c1"># Pass list of results dicts through `reduce_fn` and return a single results</span>
        <span class="c1"># dict.</span>
        <span class="k">return</span> <span class="n">reduce_fn</span><span class="p">(</span><span class="n">results</span><span class="p">)</span></div>

<div class="viewcode-block" id="Learner._update"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._update.html#ray.rllib.core.learner.learner.Learner._update">[docs]</a>    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">NestedDict</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Contains all logic for an in-graph/traceable update step.</span>

<span class="sd">        Framework specific subclasses must implement this method. This should include</span>
<span class="sd">        calls to the RLModule&#39;s `forward_train`, `compute_loss`, compute_gradients`,</span>
<span class="sd">        `postprocess_gradients`, and `apply_gradients` methods and return a tuple</span>
<span class="sd">        with all the individual results.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The train batch already converted in to a (tensor) NestedDict.</span>
<span class="sd">            kwargs: Forward compatibility kwargs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple consisting of:</span>
<span class="sd">                1) The `forward_train()` output of the RLModule,</span>
<span class="sd">                2) the loss_per_module dictionary mapping module IDs to individual loss</span>
<span class="sd">                    tensors</span>
<span class="sd">                3) a metrics dict mapping module IDs to metrics key/value pairs.</span>

<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner.set_state"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.set_state.html#ray.rllib.core.learner.learner.Learner.set_state">[docs]</a>    <span class="k">def</span> <span class="nf">set_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Set the state of the learner.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: The state of the optimizer and module. Can be obtained</span>
<span class="sd">                from `get_state`. State is a dictionary with two keys:</span>
<span class="sd">                &quot;module_state&quot; and &quot;optimizer_state&quot;. The value of each key</span>
<span class="sd">                is a dictionary that can be passed to `set_module_state` and</span>
<span class="sd">                `set_optimizer_state` respectively.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_is_built</span><span class="p">()</span>
        <span class="c1"># TODO: once we figure out the optimizer format, we can set/get the state</span>
        <span class="k">if</span> <span class="s2">&quot;module_state&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;state must have a key &#39;module_state&#39; for the module weights&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;optimizer_state&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;state must have a key &#39;optimizer_state&#39; for the optimizer weights&quot;</span>
            <span class="p">)</span>

        <span class="n">module_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;module_state&quot;</span><span class="p">)</span>
        <span class="n">optimizer_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;optimizer_state&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_module_state</span><span class="p">(</span><span class="n">module_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_optimizer_state</span><span class="p">(</span><span class="n">optimizer_state</span><span class="p">)</span></div>

<div class="viewcode-block" id="Learner.get_state"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.get_state.html#ray.rllib.core.learner.learner.Learner.get_state">[docs]</a>    <span class="k">def</span> <span class="nf">get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Get the state of the learner.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The state of the optimizer and module.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_is_built</span><span class="p">()</span>
        <span class="c1"># TODO: once we figure out the optimizer format, we can set/get the state</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;module_state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_module_state</span><span class="p">(),</span>
            <span class="s2">&quot;optimizer_state&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_optimizer_state</span><span class="p">(),</span>
        <span class="p">}</span></div>
        <span class="c1"># return {&quot;module_state&quot;: self.get_module_state(), &quot;optimizer_state&quot;: {}}</span>

<div class="viewcode-block" id="Learner.set_optimizer_state"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.set_optimizer_state.html#ray.rllib.core.learner.learner.Learner.set_optimizer_state">[docs]</a>    <span class="k">def</span> <span class="nf">set_optimizer_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Sets the state of all optimizers currently registered in this Learner.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: The state of the optimizers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Learner.get_optimizer_state"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.get_optimizer_state.html#ray.rllib.core.learner.learner.Learner.get_optimizer_state">[docs]</a>    <span class="k">def</span> <span class="nf">get_optimizer_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the state of all optimizers currently registered in this Learner.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The current state of all optimizers currently registered in this Learner.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

    <span class="k">def</span> <span class="nf">_set_slicing_by_batch_id</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">MultiAgentBatch</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentBatch</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Enables slicing by batch id in the given batch.</span>

<span class="sd">        If the input batch contains batches of sequences we need to make sure when</span>
<span class="sd">        slicing happens it is sliced via batch id and not timestamp. Calling this</span>
<span class="sd">        method enables the same flag on each SampleBatch within the input</span>
<span class="sd">        MultiAgentBatch.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The MultiAgentBatch to enable slicing by batch id on.</span>
<span class="sd">            value: The value to set the flag to.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The input MultiAgentBatch with the indexing flag is enabled / disabled on.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">policy_batch</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">policy_batches</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">[</span><span class="n">pid</span><span class="p">]</span><span class="o">.</span><span class="n">is_stateful</span><span class="p">():</span>
                <span class="c1"># We assume that arriving batches for recurrent modules are already</span>
                <span class="c1"># padded to the max sequence length and have tensors of shape</span>
                <span class="c1"># [B, T, ...]. Therefore, we slice sequence lengths in B. See</span>
                <span class="c1"># SampleBatch for more information.</span>
                <span class="k">if</span> <span class="n">value</span><span class="p">:</span>
                    <span class="n">policy_batch</span><span class="o">.</span><span class="n">enable_slicing_by_batch_id</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">policy_batch</span><span class="o">.</span><span class="n">disable_slicing_by_batch_id</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">batch</span>

<div class="viewcode-block" id="Learner._get_metadata"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._get_metadata.html#ray.rllib.core.learner.learner.Learner._get_metadata">[docs]</a>    <span class="k">def</span> <span class="nf">_get_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;learner_class&quot;</span><span class="p">:</span> <span class="n">serialize_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">),</span>
            <span class="s2">&quot;ray_version&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
            <span class="s2">&quot;ray_commit&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">__commit__</span><span class="p">,</span>
            <span class="s2">&quot;module_state_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;module_state&quot;</span><span class="p">,</span>
            <span class="s2">&quot;optimizer_state_dir&quot;</span><span class="p">:</span> <span class="s2">&quot;optimizer_state&quot;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">metadata</span></div>

<div class="viewcode-block" id="Learner._save_optimizers"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._save_optimizers.html#ray.rllib.core.learner.learner.Learner._save_optimizers">[docs]</a>    <span class="k">def</span> <span class="nf">_save_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Save the state of the optimizer to path</span>

<span class="sd">        NOTE: if path doesn&#39;t exist, then a new directory will be created. otherwise, it</span>
<span class="sd">        will be appended to.</span>

<span class="sd">        Args:</span>
<span class="sd">            path: The path to the directory to save the state to.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="Learner._load_optimizers"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._load_optimizers.html#ray.rllib.core.learner.learner.Learner._load_optimizers">[docs]</a>    <span class="k">def</span> <span class="nf">_load_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load the state of the optimizer from path</span>

<span class="sd">        Args:</span>
<span class="sd">            path: The path to the directory to load the state from.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="Learner.save_state"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.save_state.html#ray.rllib.core.learner.learner.Learner.save_state">[docs]</a>    <span class="k">def</span> <span class="nf">save_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Save the state of the learner to path</span>

<span class="sd">        NOTE: if path doesn&#39;t exist, then a new directory will be created. otherwise, it</span>
<span class="sd">        will be appended to.</span>

<span class="sd">        the state of the learner is saved in the following format:</span>

<span class="sd">        .. testcode::</span>
<span class="sd">            :skipif: True</span>

<span class="sd">            checkpoint_dir/</span>
<span class="sd">                learner_state.json</span>
<span class="sd">                module_state/</span>
<span class="sd">                    module_1/</span>
<span class="sd">                        ...</span>
<span class="sd">                optimizer_state/</span>
<span class="sd">                    optimizers_module_1/</span>
<span class="sd">                        ...</span>

<span class="sd">        Args:</span>
<span class="sd">            path: The path to the directory to save the state to.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_is_built</span><span class="p">()</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">save_to_checkpoint</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;module_state&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_save_optimizers</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;optimizer_state&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;learner_state.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_metadata</span><span class="p">()</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span></div>

<div class="viewcode-block" id="Learner.load_state"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner.load_state.html#ray.rllib.core.learner.learner.Learner.load_state">[docs]</a>    <span class="k">def</span> <span class="nf">load_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Load the state of the learner from path</span>

<span class="sd">        Note: The learner must be constructed ahead of time before its state is loaded.</span>

<span class="sd">        Args:</span>
<span class="sd">            path: The path to the directory to load the state from.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_is_built</span><span class="p">()</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module</span>
        <span class="c1"># TODO(avnishn) from checkpoint doesn&#39;t currently support modules_to_load,</span>
        <span class="c1">#  but it should, so we will add it later.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_obj</span> <span class="o">=</span> <span class="n">MultiAgentRLModule</span><span class="o">.</span><span class="n">from_checkpoint</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;module_state&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_optimizers</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;optimizer_state&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Learner._is_module_compatible_with_learner"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._is_module_compatible_with_learner.html#ray.rllib.core.learner.learner.Learner._is_module_compatible_with_learner">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_is_module_compatible_with_learner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">RLModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Check whether the module is compatible with the learner.</span>

<span class="sd">        For example, if there is a random RLModule, it will not be a torch or tf</span>
<span class="sd">        module, but rather it is a numpy module. Therefore we should not consider it</span>
<span class="sd">        during gradient based optimization.</span>

<span class="sd">        Args:</span>
<span class="sd">            module: The module to check.</span>

<span class="sd">        Returns:</span>
<span class="sd">            True if the module is compatible with the learner.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner._make_module"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._make_module.html#ray.rllib.core.learner.learner.Learner._make_module">[docs]</a>    <span class="k">def</span> <span class="nf">_make_module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiAgentRLModule</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Construct the multi-agent RL module for the learner.</span>

<span class="sd">        This method uses `self._module_specs` or `self._module_obj` to construct the</span>
<span class="sd">        module. If the module_class is a single agent RL module it will be wrapped to a</span>
<span class="sd">        multi-agent RL module. Override this method if there are other things that</span>
<span class="sd">        need to happen for instantiation of the module.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A constructed MultiAgentRLModule.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_obj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_obj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_spec</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
        <span class="c1"># If not already, convert to MultiAgentRLModule.</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">as_multi_agent</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">module</span></div>

<div class="viewcode-block" id="Learner._check_registered_optimizer"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._check_registered_optimizer.html#ray.rllib.core.learner.learner.Learner._check_registered_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">_check_registered_optimizer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Param</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Checks that the given optimizer and parameters are valid for the framework.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: The optimizer object to check.</span>
<span class="sd">            params: The list of parameters to check.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`params` (</span><span class="si">{</span><span class="n">params</span><span class="si">}</span><span class="s2">) must be a list of framework-specific parameters &quot;</span>
                <span class="s2">&quot;(variables)!&quot;</span>
            <span class="p">)</span></div>

<div class="viewcode-block" id="Learner._check_result"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._check_result.html#ray.rllib.core.learner.learner.Learner._check_result">[docs]</a>    <span class="k">def</span> <span class="nf">_check_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Checks whether the result has the correct format.</span>

<span class="sd">        All the keys should be referencing the module ids that got updated. There is a</span>
<span class="sd">        special key `ALL_MODULES` that hold any extra information that is not specific</span>
<span class="sd">        to a module.</span>

<span class="sd">        Args:</span>
<span class="sd">            result: The result of the update.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the result are not in the correct format.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The result of the update must be a dictionary. Got: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">ALL_MODULES</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The result of the update must have a key </span><span class="si">{</span><span class="n">ALL_MODULES</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;that holds any extra information that is not specific to a module.&quot;</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="n">ALL_MODULES</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The key </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> in the result of the update is not a valid &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;module id. Valid module ids are: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span></div>

<div class="viewcode-block" id="Learner._check_is_built"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._check_is_built.html#ray.rllib.core.learner.learner.Learner._check_is_built">[docs]</a>    <span class="k">def</span> <span class="nf">_check_is_built</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Learner.build() must be called after constructing a &quot;</span>
                <span class="s2">&quot;Learner and before calling any methods on it.&quot;</span>
            <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_parameters</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_named_optimizers</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_module_optimizers</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_lr_schedules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_built</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">_args</span><span class="p">,</span> <span class="o">**</span><span class="n">_kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_args</span><span class="p">,</span> <span class="o">**</span><span class="n">_kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="Learner._get_tensor_variable"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._get_tensor_variable.html#ray.rllib.core.learner.learner.Learner._get_tensor_variable">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_get_tensor_variable</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorType</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns a framework-specific tensor variable with the initial given value.</span>

<span class="sd">        This is a framework specific method that should be implemented by the</span>
<span class="sd">        framework specific sub-class.</span>

<span class="sd">        Args:</span>
<span class="sd">            value: The initial value for the tensor variable variable.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The framework specific tensor variable of the given initial value,</span>
<span class="sd">            dtype and trainable/requires_grad property.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

    <span class="nd">@staticmethod</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_get_optimizer_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the current learning rate of the given local optimizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: The local optimizer to get the current learning rate for.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The learning rate value (float) of the given optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Learner._set_optimizer_lr"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._set_optimizer_lr.html#ray.rllib.core.learner.learner.Learner._set_optimizer_lr">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_set_optimizer_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Updates the learning rate of the given local optimizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer: The local optimizer to update the learning rate for.</span>
<span class="sd">            lr: The new learning rate.</span>
<span class="sd">        &quot;&quot;&quot;</span></div>

<div class="viewcode-block" id="Learner._get_clip_function"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.Learner._get_clip_function.html#ray.rllib.core.learner.learner.Learner._get_clip_function">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_get_clip_function</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns the gradient clipping function to use, given the framework.&quot;&quot;&quot;</span></div></div>


<div class="viewcode-block" id="LearnerSpec"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.LearnerSpec.html#ray.rllib.core.learner.learner.LearnerSpec">[docs]</a><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LearnerSpec</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;The spec for constructing Learner actors.</span>

<span class="sd">    Args:</span>
<span class="sd">        learner_class: The Learner class to use.</span>
<span class="sd">        module_spec: The underlying (MA)RLModule spec to completely define the module.</span>
<span class="sd">        module: Alternatively the RLModule instance can be passed in directly. This</span>
<span class="sd">            only works if the Learner is not an actor.</span>
<span class="sd">        backend_config: The backend config for properly distributing the RLModule.</span>
<span class="sd">        learner_hyperparameters: The extra config for the loss/additional update. This</span>
<span class="sd">            should be a subclass of LearnerHyperparameters. This is useful for passing</span>
<span class="sd">            in algorithm configs that contains the hyper-parameters for loss</span>
<span class="sd">            computation, change of training behaviors, etc. e.g lr, entropy_coeff.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">learner_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="s2">&quot;Learner&quot;</span><span class="p">]</span>
    <span class="n">module_spec</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;SingleAgentRLModuleSpec&quot;</span><span class="p">,</span> <span class="s2">&quot;MultiAgentRLModuleSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;RLModule&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">learner_group_scaling_config</span><span class="p">:</span> <span class="n">LearnerGroupScalingConfig</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">LearnerGroupScalingConfig</span>
    <span class="p">)</span>
    <span class="n">learner_hyperparameters</span><span class="p">:</span> <span class="n">LearnerHyperparameters</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">LearnerHyperparameters</span>
    <span class="p">)</span>
    <span class="n">framework_hyperparameters</span><span class="p">:</span> <span class="n">FrameworkHyperparameters</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="n">FrameworkHyperparameters</span>
    <span class="p">)</span>

<div class="viewcode-block" id="LearnerSpec.get_params_dict"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.LearnerSpec.get_params_dict.html#ray.rllib.core.learner.learner.LearnerSpec.get_params_dict">[docs]</a>    <span class="k">def</span> <span class="nf">get_params_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Returns the parameters than be passed to the Learner constructor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;module&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span>
            <span class="s2">&quot;module_spec&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_spec</span><span class="p">,</span>
            <span class="s2">&quot;learner_group_scaling_config&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group_scaling_config</span><span class="p">,</span>
            <span class="s2">&quot;learner_hyperparameters&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_hyperparameters</span><span class="p">,</span>
            <span class="s2">&quot;framework_hyperparameters&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">framework_hyperparameters</span><span class="p">,</span>
        <span class="p">}</span></div>

<div class="viewcode-block" id="LearnerSpec.build"><a class="viewcode-back" href="../../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner.LearnerSpec.build.html#ray.rllib.core.learner.learner.LearnerSpec.build">[docs]</a>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Learner&quot;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Builds the Learner instance.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_class</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">get_params_dict</span><span class="p">())</span></div></div>
</pre></div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><!-- Override the footer area for the sphinx-book-theme to include the CSAT widget -->
<div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>Thanks for the feedback!</span>
  </div>
  <div id="csat-inputs">
    <span>Was this helpful?</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" fill="black"/>
      </svg>
      <span>Yes<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" fill="black"/>
      </svg>
      <span>No<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">Feedback</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">Submit</div>
  </div>
</div><p>
  
    By The Ray Team<br/>
  
      &copy; Copyright 2023, The Ray Team.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="https://docs.ray.io/en/master/_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>